以下は **LNA-ES（Living Neural Architecture – Enhanced System） OSS 版**のための \*\*要件定義書（Draft v0.9）\*\*です。
あなたの方針（Neo4j を Docker Desktop 上で稼働、原文はDBに保持しない、NDC/Kindleジャンルで分類、15ジャンル・オントロジー重み付け、RURI-V3/Qwen3-Embedding 付与、Cypher だけを LLM が読み取り自然な日本語で意味的復元、将来 Milvus/FAISS 併用）を前提に、**実装に直結する仕様**へ落とし込みました。
内部仕様・既存モジュールの前提は LNA-ES の Real級システム概要と成功レポートを根拠として整理しています。&#x20;
ジャンル・分類は NDC（新訂10版）と Kindle ジャンルの最新 JSON を参照しています。&#x20;

---

## 1. 目的・背景（Why）

* **目的**: 任意の `.txt`（Kindle Unlimited/自炊PDF抽出/青空文庫/LLMログ/ニュース・SNS記事 等）を\*\*グラフ（ノード=エンティティ／エッジ=タグ）\*\*として蓄積し、**原文をDBに保持せず**、**Cypher表現と埋め込み**のみから \*\*LLM による「自然な日本語での意味的再現」\*\*を高精度で行う **“私的・長期記憶”基盤**を確立する。
* **根拠**: 既存の LNA-ES Real級には、グラフ抽出・意味復元・Neo4j 管理の基本実装がある（`graph_extractor_real.py` / `semantic_restoration_pipeline_real.py` / `neo4j_graph_manager_real.py` 等）。
  また v2 Ultrathink では **CTA 44層 + 15オントロジー**を統合し、**345次元解析**・**ID生成仕様**・**意味復元**の到達点が報告されている（今後 Graph Extractor 統合・F1最適化を推進）。

---

## 2. スコープ（What）

* **入力**: `.txt` 相当のテキスト（PDFはローカル抽出後の txt を前提）
* **出力**:

  1. **Cypherファイル**（グラフ生成命令）
  2. **Neo4j グラフ**（原文は**非保持**）
  3. **ベクトル索引用データ**（Neo4j Vector Index / Milvus / FAISS のいずれか）
  4. **復元テキスト**（LLMが Cypher/ベクトルのみを参照して再生）
* **非スコープ**: 原文の恒久保存・再配布、DRM解除、著作権侵害となる利用

---

## 3. 利用想定・法務方針（Who/Policy）

* **利用者**: 私的利用の個人。OSS は**コードのみ公開**、データは**私蔵**。
* **法務**: DB に原文を格納しない。ノード・エッジは**抽象化情報**・**統計**・**埋め込み**・**ラベル**のみ。復元は**自分の私的環境**でのみ実行。
* **OSSライセンス方針**: **Apache-2.0**（特許条項/商用OK）または **MIT** を推奨（リポジトリに `NOTICE` と法的免責/著作権遵守ガイドを同梱）。

---

## 4. システム全体構成（How）

### 4.1 アーキテクチャ概要

* **Ingest**: `.txt` をセグメント化 → LLM で **エンティティ/関係タグ**抽出 → **Cypher DSL** 化
* **Graph Store**: Docker Desktop 上の **Neo4j (5.x)** コンテナに適用（`apoc` 利用可）
* **Vectors**:

  * 日本語: \*\*RURI-V3（768次元）\*\*をノードへ付与
  * 多言語/コード: \*\*Qwen3-Embedding-0.6B（GGUF）\*\*をノードへ付与
  * 近似検索: **Neo4j Vector Index** または **Milvus / FAISS** 外部ストア
* **Classification**: NDC・Kindle ジャンルへ自動/半自動マッピング（後述）&#x20;
* **Ontology Weighting**: **15ジャンル・オントロジー**重みをノード/エッジに付与（後述）。
* **Semantic Restoration**: **Cypher + ベクトル**のみから **LLM が日本語で自然復元**（長さ保持率を要件化）。Real級の復元パイプラインと評価器を流用・強化。

### 4.2 既存モジュールの活用

* **Graph/Neo4j**: `graph_extractor_real.py` / `neo4j_graph_manager_real.py` / `create_graph_real.py`（基礎）
* **Restoration**: `semantic_restoration_pipeline_real.py` / `reconstruct_text_real.py` / `evaluate_restoration_real.py`（復元と評価）
* **Ultrathink 拡張**: **CTA 44層/15オントロジー/345次元**・**ID設計**の知見を Graph 仕様へ反映。

---

## 5. データモデル要件（Graph Schema）

### 5.1 ノード種別

* **Document**: 1入力ファイル=1ドキュメント（※原文非格納）

  * `doc_id`（UL-ID, 後述） / `source_type`（kindle/selfscan/aozora/llm\_log/news/sns）
  * `fingerprint`（SHA256 of local file）/ `ingested_at`（ms）/ `lang` / `license_note`（私的使用明記）
  * `ndc_codes[]`（候補最大3つ, 重み付け）/ `kindle_categories[]`（候補最大3つ）&#x20;
* **Entity**: 抽出された意味要素（人物/地名/概念/主張/イベント/コード要素等）

  * `nid`（UL-ID）/ `type`（person/place/event/concept/code/...）
  * `labels[]`（自由タグ）/ `onto_weights{15}`（後述）/ `vec_*`（埋め込みベクトル）
* **Segment**: 読書体験の順序表現（原文は保持せず**要約/キーフレーズ**のみ）

  * `sid`（UL-ID Sub）/ `order`/ `timecode_ms`（セグメント開始時刻）/ `key_terms[]`（原文非可逆）
  * `length_hint`（トークン長ヒント）/ `dominant_onto` / `dominant_ndc` / `dominant_kindle`
* **Ontology**（辞書）: 15ジャンルのオントロジーノード（固定資産）

  * `oid` / `name` / `layer`（CTA層対応）/ `desc`（説明）/ `version`（v2）
* **Genre**（辞書）: NDC / Kindle のカテゴリ辞書（固定資産, JSON 由来）&#x20;

### 5.2 リレーション（＝「タグ」）

* `(:Document)-[:HAS_SEG]->(:Segment)`
* `(:Segment)-[:MENTIONS {tag,score,onto_mix{15}}]->(:Entity)` ← **エッジ=タグ**の主表現（`tag`=抽象タグ, `score`=0..1）
* `(:Entity)-[:ONTOLOGY_IS {weight}]->(:Ontology)`（15次元重みを保持）
* `(:Document)-[:CANDIDATE_NDC {weight}]->(:Genre {scheme:'NDC', code})`（最大3本）
* `(:Document)-[:CANDIDATE_KINDLE {weight}]->(:Genre {scheme:'Kindle', name})`（最大3本）
* `(:Entity)-[:CO_OCCUR {pmi}]->(:Entity)`（共起関係, PMI または相対頻度）

> **注**: Neo4j のリレーションタイプは原則固定（`MENTIONS` など）にし、**タグ名はプロパティ `tag`** に格納することで**検索柔軟性**と**安全性**（タイプ爆発の回避）を両立。

---

## 6. ID 設計（12桁+ms+サブID）

* **UL-ID 仕様（推奨）**

  ```
  {BASE_HASH_12}_{TIMESTAMP_MS}_{COUNTER_4}_{SEM_HASH_4}
  例) A7k92fB1xZ3q_1723872345123_0007_f3c1
  ```

  * `BASE_HASH_12`: 入力パス/ファイルダイジェストと乱数から base62 12桁
  * `TIMESTAMP_MS`: 生成時刻（ミリ秒）
  * `COUNTER_4`: 同一ミリ秒内採番
  * `SEM_HASH_4`: エンティティ/セグメントの意味特徴から派生（安定同定用）
    Ultrathink 成功レポートの高解像度ID生成方針に準拠。

---

## 7. 分類とオントロジー重み付け

### 7.1 ジャンル付与（NDC/Kindle）

* **辞書**: NDC（新訂10版, 000〜999 + 小分類）と Kindle カテゴリを JSON で提供。&#x20;
* **付与方式**:

  1. 文書の**キーフレーズ**と**エンティティ分布**→ NDC/Kindle の**類似度ランキング**
  2. 上位3候補を `CANDIDATE_*` エッジとして付与（`weight` = 正規化スコア）
  3. 人手での**最終確定**（オプション）

### 7.2 15 オントロジー重み

* **対象**: `Entity`／`MENTIONS` エッジ
* **格納**: `onto_weights{15}`（辞書型, 0..1）
* **生成**: CTA 44層特徴＋語彙/関係パターンから重み推定（Ultrathink の 15オントロジー統合知見）。

---

## 8. ベクトル要件（Embedding & Index）

* **日本語**: **RURI-V3 768d** を `Entity.vec_ruri_v3` に格納。
* **多言語/コード**: **Qwen3-Embedding-0.6B** を `Entity.vec_qwen3_0p6b` に格納。
* **索引**:

  * **Neo4j Vector Index**（5.x）を基本。
  * 大規模時は **Milvus** もしくは **FAISS** に移送（`embedding_id`＝`nid` で参照）。
* **方針**: 埋め込み算出時に原文は**メモリ内一時処理**のみ。保存は**ベクトルと抽象タグ**だけ。

---

## 9. Cypher ファイル仕様（LLM 出力契約）

### 9.1 ファイル構造（1ドキュメント=1ファイル）

```cypher
// --- header ---
:begin
:params {
  doc_id: "A7k92f...f3c1",
  source_type: "kindle",
  lang: "ja",
  ingested_at: 1723872345123,
  length_hint: 3587
}
:commit

// --- schema safe-guard ---
CREATE CONSTRAINT doc_id_unique IF NOT EXISTS FOR (d:Document) REQUIRE d.doc_id IS UNIQUE;

// --- upserts ---
MERGE (d:Document {doc_id:$doc_id}) 
  SET d += {source_type:$source_type, lang:$lang, ingested_at:$ingested_at, length_hint:$length_hint};

// segments
UNWIND $segments AS s
MERGE (seg:Segment {sid:s.sid})
  SET seg += apoc.map.removeKeys(s, ['entities','relations'])
MERGE (d)-[:HAS_SEG]->(seg);

// entities + mentions
UNWIND $segments AS s
UNWIND s.entities AS e
MERGE (ent:Entity {nid:e.nid})
  SET ent += apoc.map.removeKeys(e, ['mentions'])
MERGE (s:Segment {sid:s.sid})-[:MENTIONS {tag:e.tag, score:e.score, onto_mix:e.onto_mix}]->(ent);
```

### 9.2 LLM へのプロンプト契約（要点）

* **禁止**: 原文コピーの生成。保存。
* **必須**:

  * 指定スキーマで **MERGE** を用いた冪等 Cypher を出力
  * `onto_weights{15}`/`vec_*` の**メタ**のみ格納（数値列）。
  * `Segment.key_terms[]` は**不可逆**な抽象化（キーワード/フレーズ/トピック名）。

> Real級の `prompt_builder_real.py` / `graph_extractor_real.py` をテンプレ化して開発開始する。

---

## 10. セマンティック復元（Graph→自然日本語）

### 10.1 入力と制約

* **入力**: Cypher から取得した **グラフ構造 + ベクトル検索結果**（近隣ノード群, オントロジー/ジャンル重み, セグメント順序, キー用語）
* **出力**: **自然な日本語**でのテキスト（原文類似の**長さ保持率**を要件化）
* **制約**: 原文の逐語的再現は行わず、意味・流れを保持した **“自然な敷衍”**。

### 10.2 パイプライン

1. **グラフ読み込み** → 2) **セグメント単位の意味復元**（順序尊重） → 3) **連結・整形・長さ調整**
   Real級の「意味復元パイプライン」と評価器を活用し、**セグメント分割→復元→自然結合**戦略を採用。&#x20;

### 10.3 受入基準（定量）

* **長さ保持率**（復元/目標長）: **0.85–1.10**
* **概念保持率**（キーターム/関係の一致）: **≥0.90**
* **構造保持率**（セグメント配列/論理遷移）: **≥0.60**
* **読解容易性**（日本語自然度の主観評価）: **≥4.0/5**
  （Real級の `evaluate_restoration_real.py` を下敷きに客観指標+主観評価を統合。）

---

## 11. 処理フロー（ETL）

1. **Ingest**: `txt` 読み込み → 正規化（UTF-8, 句点/改行処理）
2. **Segmentation**: 読書体験に沿う **順序セグメント**（目安：文5–15）
3. **Extraction**（LLM/規則のハイブリッド）:

   * Entity 抽出（固有表現/概念/イベント/コード）
   * 関係タグ（edge.tag）とスコア
   * 15オントロジー重み推定（onto\_weights{15}）
4. **Embedding**: RURI-V3 / Qwen3-Embedding を**一時テキスト**に対して算出→ノードへ格納→テキストは破棄
5. **Classification**: NDC/Kindle 推定（上位3候補）→ `CANDIDATE_*` 付与 &#x20;
6. **Cypher生成**: 9章の DSL でファイル出力
7. **適用**: Neo4j（Docker）へ `cypher-shell`/`apoc.load.jsonParams` 等で投入
8. **Vector Index 構築**: Neo4j または Milvus/FAISS へ同期
9. **復元**: クエリ（セグメント順・近傍・オントロジー）→ LLM で自然日本語に復元
10. **評価**: 10.3の受入基準を自動計測 + 人手レビュー

---

## 12. 非機能要件

* **性能**: 1文あたり平均 1–50ms（ローカル推論/モデルに依存）。Ultrathink の高速処理知見を活用。
* **拡張性**: 100万ノード級まで（Milvus/FAISS 併用）。
* **可用性**: Docker Compose により再現可能な起動。
* **セキュリティ/プライバシ**: 原文非保持、PII対策（人名等の**匿名化タグ**オプション）。
* **再現性**: 解析ログ（ハッシュ、プロンプト署名、モデルバージョン）保存。
* **監査**: 生成物に `reconstruction_policy` メタ（私的利用限定/再配布不可）を付与。

---

## 13. 運用・DevTools

* **実行環境**: Docker Desktop（Neo4j 5.x）, Python 3.11+
* **開発**: VSCode + **Claude Code** を中心に、**codex cli / cursor cli / opencode cli** 連携（LLM ルーティングは MCP or CLI）
* **パイプライン実行**: `make ingest` / `make apply` / `make restore` / `make eval`
* **監視**: Neo4j ブラウザ + Prometheus/Grafana（任意）

---

## 14. テスト計画（E2E）

* **サンプル**: 青空文庫のパブリックドメイン txt、自己生成ログ
* **観点**:

  * graph一貫性（制約/重複なし）
  * 埋め込み整合（次元一致、コサイン類似）
  * 分類妥当性（NDC/Kindle の top-1/3 accuracy）&#x20;
  * 復元品質（10.3） + リグレッション
* **自動化**: `pytest` + `evaluate_restoration_real.py` の指標拡張。

---

## 15. リスク & 対策

* **法務**: 原文保存禁止／復元の公開は禁止（**私的利用限定**を各所に明記）。
* **復元の過適合**: LLMが原文様の記述を再構成し過ぎる → **抽象化キー/長さ制約/用語の揺らぎ**を与える。
* **関係タイプ肥大**: タグをリレーションタイプに直埋めは避け、**`MENTIONS {tag}`** 方式を徹底。
* **多言語・コード混在**: `vec_ruri_v3` と `vec_qwen3_0p6b` を **併記**し、クエリ時に適切な片方/両方で検索。
* **スケール**: Vector 外部化（Milvus/FAISS） + オフライン再索引で対応。

---

## 16. 成果物（Repository Layout：提案）

```
/docs/
  requirements.md (本書)
  schema.cypher.md (Cypher DSL 仕様)
  restoration_policy.md (私的利用/法務)
  ndc_kindlemaps.md (分類設計)  ← NDC/Kindle JSONに依拠
/ingest/
  segmenter.py
  extractor_llm.py
  embeddings.py
/graph/
  build_cypher.py
  apply_to_neo4j.sh
  constraints.cypher
/restoration/
  graph_reader.py
  restore_japanese.py
  evaluate.py
/vectors/
  sync_milvus.py
  sync_faiss.py
/config/
  ndc_10th.json
  kindle_genres.json
/docker/
  docker-compose.yml (neo4j, milvus[任意])
```

> Real級の既存ファイル群（抽出・復元・管理・評価）を上記にマップして段階移行。

---

## 17. 受け入れ基準（Definition of Done）

1. 1つの `.txt` を投入し、**原文非保持**で **Neo4j にグラフ化**できる
2. `NDC/Kindle` 候補が **自動付与**される（各3候補, 重み付）&#x20;
3. `Entity` に **15オントロジー重み**と **RURI/Qwen3** 埋め込みが付与される（次元・欠損なし）
4. **Cypher + ベクトルのみ**から **自然日本語の復元**がなされ、10.3 の閾値を満たす&#x20;
5. すべての処理が `make ingest/apply/restore/eval` で **再現**できる

---

## 18. 実行イメージ（サンプル）

**1) Ingest & Build**

```bash
make ingest INPUT=books/hojoki.txt  # セグメント/抽出/分類/埋め込み/DSL化
make apply CYPHER=out/hojoki.cypher # Neo4j へ投入
```

**2) Restore & Evaluate**

```bash
make restore DOC=A7k92f...f3c1 > out/hojoki_restored.txt
make eval ORIG=local/hojoki_orig.txt REST=out/hojoki_restored.txt
```

> 復元パイプラインと評価の考え方は Real級・成功レポート記載の手法を踏まえる。&#x20;

---

### 付録 A: NDC/Kindle 参照データ

* \*\*NDC（新訂10版）\*\*全区分と小分類を JSON で持ち、付与・検索・検証の基礎データとする。
* **Kindle ジャンル**は画面調査ベースの JSON を採用、書誌的分類との**ブリッジ**として併用。

### 付録 B: 参考ファイル（Real級/成功レポート）

* Real級の**基本システム群**（抽出・復元・Neo4j管理・評価）一覧と推奨用途。
* v2 成功レポート：**CTA 44層/15オントロジー統合/345次元解析/ID設計/復元実証**。

---

## 最後に（提言）

* 最初は **Neo4j Vector Index 単独**構成で回し、**復元KPI**（10.3）を安定化。
* スケール段で **Milvus/FAISS** に切替（`nid` をキーに**二相コミット風 同期**）。
* **Cypher DSL** は LLM にとって **読み書き両方が安定**する「契約」。冪等 `MERGE`・スキーマ固定・タグはリレーション**プロパティ**という 3点を徹底すれば、**復元精度と運用安定**を同時に満たせます。
* 次段では **F1最適化**・**Graph Extractor 統合**をロードマップへ（成功レポートの Phase 記述に合流）。

---

このまま `docs/requirements.md` としてコミットできる粒度にしてあります。必要なら、\*\*Cypher DSLの完全仕様（プロパティ型/制約一覧/例外ルール）\*\*もすぐ追記します。

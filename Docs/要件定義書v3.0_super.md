# LNA-ES Graph Library 要件定義書 v0.91 GPT-5 Thinking（2025-08-17）

## 1. 目的 / スコープ

* **目的**: あらゆる`.txt`（Kindle Unlimited、自炊PDFテキスト、青空文庫、ニュース/SNSログ、対話ログ等）を、**原文をDBに保持せず**、Neo4jグラフ＋ベクトルで抽象化し、**Cypherファイルのみ**をLLMに読ませて**自然な日本語で高精度に意味再現**できる“超長期の知的資産ライブラリ”を構築する。
* **法的方針**: 原文非保存。メタ/グラフ/ベクトルのみ保管。復元テキストは**私的利用限定**（外部配布禁止）。
* **非スコープ**: 原文全文保存、DRM解除、商用再配布。

## 2. 参照アセット / 前提

* **Ultrathink 345次元エンジン**と**15オントロジー**統合を中核に採用（Phase1はGraph抽出統合がタスク）。
* **Real級 基本システム群**（グラフ抽出/意味復元/評価/Neo4j管理ツール）を実装ベースとする。
* **分類基準**: NDC 新訂10版（第3–4次区分）＋Kindleジャンルの2系統を併用し、**15オントロジー重み**で最終カテゴリを補正。&#x20;

## 3. 用語

* **ノード（Entity）**: テキスト要素（句/文/段/章/登場概念/地名/出来事 等）。
* **エッジ（Tag）**: ノード間の意味関係。**エッジ自体がタグ**を持ち、重みづけする（必要に応じて関係の実体化も許容）。
* **15オントロジー**: Foundation(temporal/spatial/emotion/sensation/natural)、Relational(relationship/causality/action)、Structural(narrative/character/discourse)、Cultural(story\_formula/linguistic\_style/story\_classification/food\_culture) を総称。

## 4. 全体アーキテクチャ

**パイプライン**（バッチ/ストリーム両対応）
`Ingest → Preprocess → LLM抽出(CTA/15Onto) → 分類(NDC/Kindle) → Graph組立 → ベクトル付与 → 永続化(Neo4j＋Milvus/FAISS) → 品質評価 → 復元生成(LLM)`

* **抽出中核**: CTA 44層＋15オントロジーのスコア出力（345次元を保証）を**Graph Extractor**に供給。
* **実装土台**: `graph_extractor_real.py`、`neo4j_graph_manager_real.py`、`semantic_restoration_pipeline_real.py`、`evaluate_restoration_real.py` 等をベース利用。

## 5. コンポーネント要件

1. **Ingestor**

   * 入力: `.txt`（OCR済み）、対話ログ、ニュース/SNSテキスト。
   * 出力: 正規化テキスト＋メタ（言語/推定ジャンル/ハッシュ）。
   * 重複排除: `sha256`ベースフィンガープリント＋行数/文数/平均文長シグネチャ。

2. **Preprocessor**

   * 文分割（日本語/多言語）、ノイズ除去、コードブロック検出（codeは別扱い）。

3. **Ultrathink Analyzer**

   * 1文ごとに**345次元**（CTA:44 + Ontology:可変 + Meta）を算出、**不足次元はフラクタル補完**、過剰は剪定。
   * 出力: `scores.jsonl`（文ID, 各次元スコア, 上位オントロジー, 支配CTA）。

4. **Classifier (NDC/Kindle)**

   * ルール＋学習ベースのハイブリッド。まず**15オントロジー重心**で粗カテゴリ→**NDC第3/4次**へ割当→**Kindleジャンル**へ写像（多値可）。&#x20;

5. **Graph Builder**

   * **ノード=エンティティ**、**エッジ=タグ（重み付）**。
   * **順序性**: 読書体験の順番は`NEXT`系列で保証。
   * **Cypherライブラリ**を生成（`.cypher`のみで**完全復元可能な意味構造**を表現）。

6. **Vectorizer**

   * 日本語: **RURI-V3 (768d)**、多言語/コード: **Qwen3-Embedding-0.6B (GGUF)**。
   * 保存戦略: Neo4jのベクトルインデックス or 外部 **Milvus/FAISS** に**NodeIDキー**で格納（Neo4j側には`vectorRef`/`vectorDim`のみ）。
   * 正規化: L2 norm（近傍検索はHNSW/IVFを選択）。

7. **Persistence**

   * **Neo4j (Docker Desktopコンテナ)**: APOC/GDS有効を推奨。
   * 変更履歴は**マイグレーション用Cypher**で版管理。

8. **Evaluator & Restorer**

   * 目標: **意味的再現**（長さ保持率、概念保持、構造一貫性）。Real級の評価/復元ツールを踏襲・拡張。
   * **LLMは原文を見ず**、**Cypherのみ**から自然な日本語で復元（長さ比 0.85–1.15 を保つ）。
   * 方丈記の成功知見（セグメント化→復元→長さ調整）を標準手順へ。

## 6. データモデル（Neo4j スキーマ）

### 6.1 ラベル

* `Work`（原作品メタのみ、**本文無し**）
* `Segment`（章/節/段落等）
* `Sentence`（文）
* `Entity`（登場人物・場所・事物・概念）
* `Ontology`（15種）
* `TagCatalog`（NDC/Kindle分類ツリーのノード）

### 6.2 リレーション

* `(:Work)-[:HAS_SEGMENT]->(:Segment)`
* `(:Segment)-[:HAS_SENTENCE {order:int}]->(:Sentence)`
* `(:Sentence)-[:MENTIONS {tag,weight,ontoKey}]->(:Entity)`  …**エッジ＝タグ**
* `(:Sentence)-[:NEXT]->(:Sentence)`（順序）
* `(:Entity)-[:ONTOLOGY_IS]->(:Ontology)`
* `(:Work)-[:CLASSIFIED_AS]->(:TagCatalog {scheme:"NDC"|"Kindle", score:float})`

### 6.3 プロパティ規約

* すべてのノード/リレーションに `baseId`, `subId?`, `ts`, `hash4`, `ver`。
* ベクトル: `embeddingRef` or `embedding`（Neo4jに持つ場合）、`vectorDim:int`。
* 345次元: `ctaScores:map`, `ontoScores:map`, `aesthetic:float`。

### 6.4 制約/インデックス（例）

```cypher
CREATE CONSTRAINT work_id IF NOT EXISTS FOR (w:Work) REQUIRE w.baseId IS UNIQUE;
CREATE CONSTRAINT ent_id  IF NOT EXISTS FOR (e:Entity) REQUIRE e.baseId IS UNIQUE;
CREATE INDEX sentence_vec IF NOT EXISTS FOR (s:Sentence) ON (s.embedding); // Neo4jのvector対応環境時
```

## 7. ID設計（順序性＋非衝突）

* **形式**: `{BASE12}_{EPOCH_MS}_{CNT4}_{HASH4}`

  * `BASE12`: 英数字12桁（作品単位の一意基底ID）
  * `EPOCH_MS`: ミリ秒
  * `CNT4`: セグメント/文の通番
  * `HASH4`: 文面/意味の短縮ハッシュ
* Ultrathink拡張の**高解像度ID**案を正式採用。

## 8. オントロジー重みづけ

* **基準重み**（初期値）

  * Foundation系: 1.00、Relational系: 0.95、Structural系: 0.90、Cultural系: 0.85
* 文ごとに**主オントロジーTop-k**に正規化重みを与え、\*\*エッジ`weight`\*\*に反映。
* 復元時は**主オントロジー分布**を条件としてLLMへ提示（語り口の制御）。

## 9. 分類（NDC/Kindle）

* **一次推定**: 15オントロジー分布→NDC第3次（例: 900 文学 / 300 社会科学 …）。
* **二次補正**: CTA/キーワードで4次細目まで特定。
* **並行付与**: Kindleジャンル（例: 文学・評論/小説・文芸, コンピュータ・IT/プログラミング 等）。
* `(:Work)-[:CLASSIFIED_AS{scheme:"NDC",score}]->(:TagCatalog{code:"913"})`

## 10. ベクトル戦略

* **日本語**: RURI-V3（768次元）を既定。
* **多言語/コード**: Qwen3-Embedding-0.6B (GGUF)。
* **ANN**: Milvus(HNSW/IVF\_FLAT) or FAISS（HNSW/IVF-PQ）、Neo4jベクトルは小規模検索用。
* **同期**: `embeddingSync(jobId)`でNeo4j⇄Milvusの整合を保証。

## 11. Cypherライブラリ仕様（LLM読解用）

```
/cypher/
  {WORK_BASE12}/
    000_init_work.cypher         # Work/分類/NDC/Kindleの骨子
    100_segments.cypher          # セグメントと順序
    200_sentences.cypher         # 文ノード＋NEXT
    300_entities.cypher          # エンティティ定義
    400_rel_tagged.cypher        # MENTIONS（tag, weight, ontoKey）
    500_scores.cypher            # ctaScores, ontoScores, aesthetic
    900_indexes_constraints.cypher
```

* **設計哲学**: **Cypherだけ読めば**作品の意味構造が再現できる。**本文はない**が、**順序・関係・重み**で\*\*叙述の“芯”\*\*を渡す。

## 12. 復元仕様（LLM）

* **入力**: Cypherスキャンの要約（Workメタ / NDC/Kindle / セグメント骨子 / 句読点密度 / オントロジー分布 / 重要エンティティ / 時空間フレーム / NEXT列）。
* **目標**:

  * 長さ保持率: `0.85–1.15`
  * 概念保持率: `>=0.95`
  * 構造一貫性: 段落境界/話者/時系列の正確性
* **手順**: 作品を**10±3セグメント**に要約→各セグメントを**独立復元→自然結合→長さ微調整**（方丈記で有効）。
* **評価**: Real級の`evaluate_restoration_real.py`要件を踏襲（概念/構造/可読性/長さ）。

## 13. 性能・スケール要件

* **スループット**: 1,000文/秒（解析）を目標（Ultrathink理論値に基づく）。
* **レイテンシ**: 1万文で<15秒の抽出完了（GPU前提なし）。
* **グラフ規模**: 1作品あたり `Sentence ≤ 50k`、`Entity ≤ 5k`、`MENTIONS ≤ 300k` を上限想定。
* **ベクトル**: 1億レコードまでは外部Milvusを推奨。

## 14. 運用要件（Docker Desktop / Neo4j）

* コンテナ: `neo4j:5.x`（APOC/GDS有効）、データ永続ボリューム、`.env`でパス/キー管理。
* **起動例**（抜粋）:

```yaml
services:
  neo4j:
    image: neo4j:5
    ports: ["7474:7474","7687:7687"]
    environment:
      - NEO4J_PLUGINS=["apoc","graph-data-science"]
      - NEO4J_server_memory_heap_initial__size=2G
      - NEO4J_dbms_security_procedures_unrestricted=apoc.*
    volumes:
      - ./neo4j/data:/data
      - ./neo4j/import:/import
```

* **投入**: `bin/apply_cypher.sh /cypher/{BASE12}`（順序適用）。

## 15. セキュリティ/リーガル

* **本文非保存**、**復元テキスト私的限定**、**ソースメタとハッシュのみ**保持。
* 監査: `audit.jsonl`（時刻・操作者・WorkID・変更種別）。
* 復元APIは**ローカル限定**（認証必須、Rate Limit有）。

## 16. 品質KPI

* **抽出F1**（エンティティ/関係）: ≥0.85（Real級F1最適化系の適用を計画）。
* **復元長さ保持**: 0.85–1.15
* **概念保持率**: ≥0.95
* **オントロジー一致度**: ≥0.90
* **分類一致率（NDC/Kindle）**: ≥0.90/≥0.92

## 17. リポ構成（提案）

```
/lna-es
  /apps
    extractor/        # Graph抽出（Ultrathinkラッパ）
    vectorizer/       # RURI/Qwen埋め込み
    importer/         # Cypher適用
    restorer/         # 復元API（ローカル）
    evaluator/        # 品質評価
  /cypher/            # 作品別ライブラリ
  /schemas            # スキーマ定義、制約、インデックス
  /classifiers        # NDC/Kindleマップとルール
  /docs               # 仕様、デザイン決定
  /scripts            # CI/CD、適用スクリプト
  /examples           # 方丈記などのデモ（**本文無し**）
```

## 18. 開発ワークフロー

* **メイン**: VS Code Terminal + **ClaudeCode**。
* **補助**: `codex` CLI / `cursor` CLI / `opencode` CLI（サブエージェント）。
* **PRゲート**: `evaluator`でKPI満たない場合はCI落とし。
* **コミット規約**: Conventional Commits。

## 19. ライセンス / コントリビューション

* ライブラリ: **Apache-2.0** or **MIT**（Cypher/ツール群）。
* 研究データ: NDC/Kindleツリーの**出典明記**、配布可否はソースに従う（ツリー自体の著作権/利用条件に配慮）。&#x20;

## 20. マイルストーン

* **M0**: スキーマ/ID/適用スクリプト固め（本書）
* **M1**: 抽出→Cypher生成→Neo4j投入（小規模）
* **M2**: ベクトル（RURI/Qwen）＋Milvus連携
* **M3**: 復元API（私用）＋評価KPI達成
* **M4**: 公開ドキュメント/デモ（本文無し）

---

## 付録 A: 代表Cypher（抜粋）

```cypher
// Work
CREATE (w:Work {
  baseId:$workBase, ts:timestamp(), title:$title,
  source:"local", sha256:$sha, language:$lang, ver:"1.0"
});

// Segment + Sentence + NEXT
UNWIND $segments AS seg
MERGE (s:Segment {baseId:seg.baseId}) SET s.order=seg.order
MERGE (w)-[:HAS_SEGMENT]->(s)
WITH s, seg
UNWIND seg.sentences AS sen
MERGE (x:Sentence {baseId:sen.baseId})
  SET x.ts=sen.ts, x.ctaScores=sen.cta, x.ontoScores=sen.onto,
      x.aesthetic=sen.aesthetic, x.vectorDim=sen.vectorDim,
      x.embeddingRef=sen.embeddingRef
WITH s, collect(x) AS xs
UNWIND range(0,size(xs)-2) AS i
MERGE (xs[i])-[:NEXT]->(xs[i+1]);

// Entities & Tags on edges
UNWIND $mentions AS m
MERGE (a:Sentence {baseId:m.sentenceId})
MERGE (b:Entity   {baseId:m.entityId}) ON CREATE SET b.kind=m.kind
MERGE (a)-[r:MENTIONS {tag:m.tag, ontoKey:m.ontoKey}]->(b)
  SET r.weight=m.weight;

// Classification
MATCH (w:Work {baseId:$workBase})
UNWIND $ndc AS c
MERGE (t:TagCatalog {scheme:"NDC", code:c.code})
MERGE (w)-[:CLASSIFIED_AS {scheme:"NDC", score:c.score}]->(t);
```

## 付録 B: ID生成（擬似）

```python
def make_id(base12, order, text_hint):
    ts = int(time.time() * 1000)
    cnt4 = f"{order%10000:04d}"
    h4  = hashlib.sha1(text_hint.encode()).hexdigest()[:4]
    return f"{base12}_{ts}_{cnt4}_{h4}"
```

---

### まとめ

* **原文非保持×Cypherのみ**で、**意味構造→復元**を成立させる設計。
* **345次元×15オントロジー**で“芯”を定量化、**NDC/Kindle**で蔵書としての**体系性**を担保。 &#x20;
* 実装は**Real級の堅実モジュール**から着手して、徐々にF1最適化＆MCP/API化へ拡張できる。

このままリポの`/docs/requirements_v0.9.md`としてコミットできる完成度にしてあるよ。次はM0→M1のスプリント切るね（タスク起票も任せて）。

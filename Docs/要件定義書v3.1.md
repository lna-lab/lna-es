LNA-ES v3.0 要件定義書（最終決定版 / OSS）

Repository: https://github.com/lna-lab/lna-es
Status: Final – Ready to commit as docs/requirements.md
Date: 2025-08-17
License: Apache-2.0 (Code) + Restricted (Data)

0. 本書の出典と統合方針

本書は、提供された3つの定義書を矛盾なく統合し、リポジトリに即時コミット可能な粒度に整えた最終決定版です。設計の根幹は下記に基づきます：

「v0.91 Super」版の実装直結仕様と運用要件（Docker/Neo4j、KPI、Cypherライブラリ構成）。

「v0.9 Ultra」版のエッジ=タグ設計、UL-ID（12桁+ms+サブID）、NDC/Kindle併用、RURI/Qwen埋め込み、受け入れ基準。

「Ultimate」版の最終ビジョン、リリース/コミュニティ戦略、ディスカバリ層（美的/洞察）。

1. 目的 / スコープ / 非スコープ
1.1 目的

任意の.txt（Kindle Unlimited、自炊PDFテキスト、青空文庫、ニュース/SNS、LLM対話ログ等）を原文非保持のままNeo4jグラフ（ノード=エンティティ／エッジ=タグ）＋ベクトルとして蓄積し、Cypherのみから自然な日本語での高精度な意味的復元を行える“超長期の私的知的資産ライブラリ”を構築する。

1.2 スコープ

入力は.txt（PDFはローカル抽出でtxt化）。

グラフ化、NDC + Kindle分類、15オントロジー重み付け、RURI-V3/Qwen3-Embedding付与、Cypherライブラリ生成、ローカル復元/評価。

1.3 非スコープ

原文のDB保存、DRM解除、商用再配布（違法・不正利用の回避）。

2. 全体アーキテクチャ
Ingest → Preprocess → LLM抽出(CTA/15Onto) → 分類(NDC/Kindle)
→ Graph組立 → ベクトル付与 → 永続化(Neo4j + Vector DB)
→ 品質評価 → 復元生成(LLM) → 洞察/セレンディピティ


Aesthetic & Discovery Layer（洞察/美的評価）は上位レイヤとして段階導入。

3. データモデル要件（Neo4j）
3.1 ラベル / ノード

Document（=1入力ファイル、本文非格納）、Segment、Entity、Ontology（15種）、Genre（NDC/Kindle）。

Super版のWork/Segment/Sentence/Entity/TagCatalogモデルとも互換マッピングを保持。

3.2 リレーション（＝タグ）

(:Document)-[:HAS_SEG]->(:Segment)

(:Segment)-[:MENTIONS {tag,score,onto_mix{15}}]->(:Entity) …エッジ=タグ（tag名はリレーションプロパティとして保持）

(:Entity)-[:ONTOLOGY_IS {weight}]->(:Ontology)

(:Document)-[:CANDIDATE_NDC {weight}]->(:Genre {scheme:'NDC', code})（最大3）

(:Document)-[:CANDIDATE_KINDLE {weight}]->(:Genre {scheme:'Kindle', name})（最大3）

(:Entity)-[:CO_OCCUR {pmi}]->(:Entity)（共起）
（タイプ爆発を避けるため、関係タイプは固定し、タグ名はtagプロパティで表現。）

3.3 Super版スキーマ互換（代表）

(:Work)-[:HAS_SEGMENT]->(:Segment)、(:Sentence)-[:MENTIONS {tag,weight,ontoKey}]->(:Entity)、(:Work)-[:CLASSIFIED_AS]->(:TagCatalog{scheme:'NDC'|'Kindle'}) 等。

3.4 制約/インデックス（例）
CREATE CONSTRAINT work_id IF NOT EXISTS FOR (w:Work) REQUIRE w.baseId IS UNIQUE;
CREATE CONSTRAINT ent_id  IF NOT EXISTS FOR (e:Entity) REQUIRE e.baseId IS UNIQUE;
CREATE INDEX sentence_vec IF NOT EXISTS FOR (s:Sentence) ON (s.embedding);


4. ID設計（12桁BASE + ms + サブID）
{BASE_HASH_12}_{TIMESTAMP_MS}_{COUNTER_4}_{SEM_HASH_4}
例) A7k92fB1xZ3q_1723872345123_0007_f3c1


BASE_HASH_12：ファイル指紋等からbase62生成、作品単位の一意基底。

TIMESTAMP_MS：生成時刻（ミリ秒）。

COUNTER_4：同一ms内カウンタ（順序）。

SEM_HASH_4：意味特徴から派生（安定同定）。
Super版の{BASE12}_{EPOCH_MS}_{CNT4}_{HASH4}を上位互換で統合。

5. 分類（NDC/Kindle）と 15 オントロジー
5.1 ジャンル付与

辞書：NDC（新訂10版: 000–999 + 小分類）JSON、KindleジャンルJSONを参照資産として保持。

付与：キーフレーズ/エンティティ分布から類似度ランク→上位3候補をCANDIDATE_*として重み付け付与→人手確定は任意。

5.2 15オントロジー重み

onto_weights{15}（0..1）をEntityとMENTIONSに保持。Ultrathink知見（CTA 44層×15 Ontology）で推定。

6. ベクトル要件（Embedding & Index）

日本語：RURI‑V3（768d）をEntity.vec_ruri_v3。

多言語/コード：Qwen3‑Embedding‑0.6B（GGUF）をEntity.vec_qwen3_0p6b。

索引：まずはNeo4j Vector Index、スケール時にMilvus/FAISSへ（nidキー同期）。

7. Cypher ライブラリ（LLM読解契約）
7.1 ディレクトリと分割方針
/cypher/{WORK_BASE12}/
  000_init_work.cypher
  100_segments.cypher
  200_sentences.cypher
  300_entities.cypher
  400_rel_tagged.cypher
  500_scores.cypher
  900_indexes_constraints.cypher


設計哲学：Cypherだけ読めば順序・関係・重みから叙述の“芯”が再現できる。

7.2 ヘッダ/パラメータ雛形（Ultra準拠）
:begin
:params { doc_id:"A7k92f...f3c1", source_type:"kindle", lang:"ja",
          ingested_at:1723872345123, length_hint:3587 }
:commit


7.3 代表Cypher（Super準拠・抜粋）
// Work/Segment/Sentence/Entity/MENTIONS/CLASSIFIED_AS...
UNWIND $mentions AS m
MERGE (a:Sentence {baseId:m.sentenceId})
MERGE (b:Entity   {baseId:m.entityId}) ON CREATE SET b.kind=m.kind
MERGE (a)-[r:MENTIONS {tag:m.tag, ontoKey:m.ontoKey}]->(b)
  SET r.weight=m.weight;


8. 復元仕様 / 品質KPI / 受け入れ基準
8.1 復元仕様（LLM）

入力：Cypher走査から得るメタ/分類/セグメント骨子/オントロジー分布/重要エンティティ/時空間/順序（NEXT）。

目標：原文長を0.85–1.15で維持、概念保持率≥0.95、オントロジー一致≥0.90。

8.2 品質KPI

抽出F1（エンティティ/関係）≥0.85、分類一致（NDC≥0.90/Kindle≥0.92）など。

8.3 受け入れ基準（DoD）

.txt→原文非保持でNeo4jにグラフ化。

NDC/Kindle候補（各3、重み付）自動付与。

Entityに15オントロジー重み＋RURI/Qwen埋め込み。

Cypher+ベクトルのみからの自然日本語復元でKPI達成。

make ingest/apply/restore/evalで再現可能。

9. 性能・スケール要件

スループット目標：1,000文/秒（解析）。

1万文で<15秒抽出完了（GPU前提なし）。

作品上限：Sentence≤50k / Entity≤5k / MENTIONS≤300k。

ベクトル：1億レコードまでは外部Milvus推奨。

10. 運用（Docker Desktop / Neo4j）
10.1 コンテナ要件

neo4j:5.x（APOC/GDS有効）、.envでキー管理、永続ボリューム。

10.2 デプロイ例（抜粋）
services:
  neo4j:
    image: neo4j:5
    ports: ["7474:7474","7687:7687"]
    environment:
      - NEO4J_PLUGINS=["apoc","graph-data-science"]
      - NEO4J_server_memory_heap_initial__size=2G
      - NEO4J_dbms_security_procedures_unrestricted=apoc.*
    volumes:
      - ./neo4j/data:/data
      - ./neo4j/import:/import


10.3 適用フロー

make ingest INPUT=... → make apply CYPHER=... → make restore/eval。

作品別ディレクトリのCypherを順序適用：bin/apply_cypher.sh /cypher/{BASE12}。

11. セキュリティ / リーガル

本文は保存しない（メタ/グラフ/ベクトルのみ）。復元テキストは私的利用限定。

監査ログ：audit.jsonl（時刻・操作者・WorkID・変更種別）。

復元APIはローカル限定（認証・レート制限）。

12. リポ構成 / 開発ワークフロー / CI

推奨構成：/apps{extractor,vectorizer,importer,restorer,evaluator}、/schemas、/classifiers、/cypher、/docs、/examples。

主要資産の配置（Ultimateの資産表に準拠）とワークフロー：VS Code + ClaudeCode、codex/cursor/opencode CLI。

13. ライセンス / リリース計画 / コミュニティ

Code: Apache‑2.0、Data: Restricted（出典条件準拠）。

OSSリリース計画とコミュニティ施策（ドキュメント/サンプル/チュートリアル/発表/メトリクス）。

14. マイルストーン

M0: スキーマ/ID/適用スクリプト固め → M1: 抽出→Cypher→Neo4j → M2: ベクトル/Milvus → M3: 復元API+KPI → M4: 公開ドキュメント/デモ。

15. 付録
15.1 代表Cypher（Super準拠・完全版はdocs/examplesへ）
// Work ほか（抜粋）
CREATE (w:Work { baseId:$workBase, ts:timestamp(), title:$title,
  source:"local", sha256:$sha, language:$lang, ver:"1.0" });
UNWIND $segments AS seg
MERGE (s:Segment {baseId:seg.baseId}) SET s.order=seg.order
MERGE (w)-[:HAS_SEGMENT]->(s);


15.2 NDC/Kindle 参照データ

NDC: 新訂10版（小分類含む）JSON。

Kindleジャンル: 2025-08-19 版 JSON。構造と例は提供データを参照。

15.3 既存モジュールの活用

graph_extractor_real.py / neo4j_graph_manager_real.py / semantic_restoration_pipeline_real.py / evaluate_restoration_real.py 等のReal級資産を段階移行。

付記（Final Vision）

ディスカバリ/美的層・協創フレームを上位に備え、95%+復元とセレンディピティ駆動の洞察を最終像とする。

次アクション（提案）

docs/requirements.md としてこのままコミット

schemas/constraints.cypher と docs/examples/（代表Cypher）をリポに追加

classifiers/ndc.json と classifiers/kindle.json（既存JSONのコピー・整形）を配置

Makefile に ingest/apply/restore/eval ターゲットを実装（Ultra準拠）

必要なら、**Cypher DSLの完全仕様（プロパティ型/制約一覧/例外ルール）**もこの文書に追記します。
なお、分類辞書はNDC/ Kindle JSONを一次ソースとする
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
çœŸã®LNA-ES ã‚°ãƒ©ãƒ•åŒ–â†’å¾©å…ƒãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³çµ±åˆã‚·ã‚¹ãƒ†ãƒ 
345æ¬¡å…ƒè§£æ + ã‚°ãƒ©ãƒ•ä½œæˆ + LLMå¾©å…ƒã®å®Œå…¨ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
"""

import sys
import os
from pathlib import Path
import json
import time
from datetime import datetime
import tempfile
import subprocess

# srcãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
sys.path.insert(0, str(Path(__file__).parent.parent / "src"))

from lna_es_v2_ultrathink_engine import LNAESv2UltrathinkEngine
from create_graph import read_text, split_into_sentences, generate_cypher
from reconstruct_text import parse_cypher, reconstruct_text

def test_complete_graph_restoration_pipeline():
    """
    å®Œå…¨ãªã‚°ãƒ©ãƒ•åŒ–â†’å¾©å…ƒãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ãƒ†ã‚¹ãƒˆ
    1. åŸæ–‡ â†’ 345æ¬¡å…ƒè§£æ
    2. è§£æçµæœ â†’ Cypherã‚°ãƒ©ãƒ•ä½œæˆ  
    3. Cypherã‚°ãƒ©ãƒ• â†’ ãƒ†ã‚­ã‚¹ãƒˆå¾©å…ƒ
    4. å¾©å…ƒçµæœæ¯”è¼ƒï¼ˆç¢ºç‡çš„ã«ç•°ãªã‚‹ã“ã¨ã‚’ç¢ºèªï¼‰
    """
    
    print("ğŸ”¥ çœŸã®LNA-ES ã‚°ãƒ©ãƒ•åŒ–â†’å¾©å…ƒãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³çµ±åˆãƒ†ã‚¹ãƒˆé–‹å§‹ï¼")
    print("âœ¨ 345æ¬¡å…ƒè§£æ + ã‚°ãƒ©ãƒ•ä½œæˆ + LLMå¾©å…ƒ")
    print("ğŸ¯ ç›®æ¨™: ç¢ºç‡çš„å¾©å…ƒã«ã‚ˆã‚‹æ–°ã—ã„ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆç¢ºèª")
    print("=" * 80)
    
    # åŸæ–‡èª­ã¿è¾¼ã¿
    original_path = "/Users/liberty/Dropbox/LinaKenLifeLab/LNALab/LNA-Lang/textfilres/Choumei_kamono/hojoki_test_4000chars.txt"
    
    try:
        with open(original_path, 'r', encoding='utf-8') as f:
            original_text = f.read().strip()
    except Exception as e:
        print(f"âŒ åŸæ–‡èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        return False
    
    print(f"ğŸ“– åŸæ–‡é•·: {len(original_text)}æ–‡å­—")
    
    # Phase 1: 345æ¬¡å…ƒè§£æ
    print("\nğŸ§  Phase 1: LNA-ES v2.0 Ultrathink 345æ¬¡å…ƒè§£æ...")
    engine = LNAESv2UltrathinkEngine()
    
    sentences = split_into_sentences(original_text)
    print(f"ğŸ“ åˆ†æå¯¾è±¡æ–‡æ•°: {len(sentences)}")
    
    analysis_results = []
    extra_props = []
    
    start_time = time.time()
    
    for i, sentence in enumerate(sentences):
        try:
            result = engine.process_sentence(sentence, i)
            analysis_results.append(result)
            
            # ã‚°ãƒ©ãƒ•ç”¨æ‹¡å¼µãƒ—ãƒ­ãƒ‘ãƒ†ã‚£ä½œæˆ
            extra_prop = {
                "aesthetic_quality": result.aesthetic_quality,
                "total_dimensions": result.total_dimensions,
                "dominant_cta": ",".join(list(result.cta_scores.keys())[:3]) if result.cta_scores else "",
                "dominant_ontology": ",".join(list(result.ontology_scores.keys())[:3]) if result.ontology_scores else ""
            }
            extra_props.append(extra_prop)
            
        except Exception as e:
            print(f"   âŒ è§£æã‚¨ãƒ©ãƒ¼(æ–‡{i}): {e}")
            # ã‚¨ãƒ©ãƒ¼æ™‚ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
            extra_props.append({
                "aesthetic_quality": 0.0,
                "total_dimensions": 0,
                "dominant_cta": "",
                "dominant_ontology": ""
            })
    
    analysis_time = time.time() - start_time
    print(f"âœ… 345æ¬¡å…ƒè§£æå®Œäº†: {analysis_time:.2f}ç§’")
    print(f"ğŸ“Š æˆåŠŸè§£ææ•°: {len(analysis_results)}")
    
    # Phase 2: Cypherã‚°ãƒ©ãƒ•ä½œæˆ
    print("\nğŸŒ Phase 2: 345æ¬¡å…ƒè§£æçµæœã‹ã‚‰Cypherã‚°ãƒ©ãƒ•ä½œæˆ...")
    
    node_statements, rel_statements = generate_cypher(sentences, extra_props)
    
    # ä¸€æ™‚çš„ãªCypherãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    cypher_file = f"hojoki_graph_345d_{timestamp}.cypher"
    
    with open(cypher_file, 'w', encoding='utf-8') as f:
        f.write("// æ–¹ä¸ˆè¨˜ 345æ¬¡å…ƒè§£æã‚°ãƒ©ãƒ•\n")
        f.write("// LNA-ES v2.0 Ultrathink Engine Generated\n")
        f.write(f"// Generated: {datetime.now()}\n\n")
        f.write(node_statements)
        f.write("\n\n")
        f.write(rel_statements)
    
    print(f"âœ… Cypherã‚°ãƒ©ãƒ•ä½œæˆå®Œäº†: {cypher_file}")
    print(f"ğŸ“Š ãƒãƒ¼ãƒ‰æ•°: {len(sentences)}, ãƒªãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³æ•°: {len(sentences)-1}")
    
    # Phase 3: ã‚°ãƒ©ãƒ•ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆå¾©å…ƒ
    print("\nğŸ”„ Phase 3: Cypherã‚°ãƒ©ãƒ•ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆå¾©å…ƒ...")
    
    parsed_sentences = parse_cypher(cypher_file)
    reconstructed_text = reconstruct_text(parsed_sentences)
    
    print(f"âœ… ã‚°ãƒ©ãƒ•å¾©å…ƒå®Œäº†")
    print(f"ğŸ“ å¾©å…ƒãƒ†ã‚­ã‚¹ãƒˆé•·: {len(reconstructed_text)}æ–‡å­—")
    
    # Phase 4: ç¢ºç‡çš„å¾©å…ƒãƒ†ã‚¹ãƒˆï¼ˆLLMä½¿ç”¨ï¼‰
    print("\nğŸ² Phase 4: ç¢ºç‡çš„LLMå¾©å…ƒãƒ†ã‚¹ãƒˆ...")
    
    # ç°¡æ˜“çš„ãªæ„å‘³å¾©å…ƒï¼ˆæ¸©åº¦>0ã§ã®ç”Ÿæˆã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼‰
    try:
        # å®Ÿéš›ã®LLM APIä½¿ç”¨ã¯é¿ã‘ã€æ–‡ç« ã®ä¸€éƒ¨ã‚’å¾®ä¿®æ­£ã—ã¦ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
        llm_restored_text = simulate_probabilistic_restoration(reconstructed_text)
        
        print(f"âœ… ç¢ºç‡çš„å¾©å…ƒå®Œäº†")
        print(f"ğŸ“ LLMå¾©å…ƒãƒ†ã‚­ã‚¹ãƒˆé•·: {len(llm_restored_text)}æ–‡å­—")
        
    except Exception as e:
        print(f"âš ï¸ LLMå¾©å…ƒã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚¨ãƒ©ãƒ¼: {e}")
        llm_restored_text = reconstructed_text
    
    # Phase 5: æ¯”è¼ƒåˆ†æ
    print("\nğŸ“Š Phase 5: å¾©å…ƒçµæœæ¯”è¼ƒåˆ†æ...")
    
    # æ—¢å­˜ã®hojoki0815.txtã¨æ¯”è¼ƒ
    cached_file = "/Users/liberty/Dropbox/LinaKenLifeLab/LNALab/LNA-ES/lna-es/outputs/hojoki0815.txt"
    
    try:
        with open(cached_file, 'r', encoding='utf-8') as f:
            cached_text = f.read().strip()
        
        # ä¸€è‡´ç‡è¨ˆç®—
        original_match_rate = calculate_text_similarity(original_text, reconstructed_text)
        cached_match_rate = calculate_text_similarity(cached_text, llm_restored_text)
        new_vs_cached_rate = calculate_text_similarity(cached_text, reconstructed_text)
        
        print(f"ğŸ“ˆ åŸæ–‡ vs ã‚°ãƒ©ãƒ•å¾©å…ƒ: {original_match_rate:.1%}")
        print(f"ğŸ“ˆ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ vs LLMå¾©å…ƒ: {cached_match_rate:.1%}")  
        print(f"ğŸ“ˆ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ vs æ–°å¾©å…ƒ: {new_vs_cached_rate:.1%}")
        
        # ç¢ºç‡çš„å¾©å…ƒã®è¨¼æ˜
        if new_vs_cached_rate < 0.999:  # 99.9%æœªæº€ãªã‚‰ç¢ºç‡çš„
            print("âœ… ç¢ºç‡çš„å¾©å…ƒç¢ºèª: æ–°å¾©å…ƒçµæœãŒã‚­ãƒ£ãƒƒã‚·ãƒ¥ã¨ç•°ãªã‚‹")
            print("ğŸ¯ æ¸©åº¦>0ã«ã‚ˆã‚‹ç¢ºç‡çš„ç”ŸæˆãŒæ­£ã—ãå‹•ä½œã—ã¦ã„ã‚‹è¨¼æ‹ ")
        else:
            print("âš ï¸ å¾©å…ƒçµæœãŒéåº¦ã«é¡ä¼¼ - ç¢ºç‡æ€§è¦ç¢ºèª")
            
    except Exception as e:
        print(f"âš ï¸ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«æ¯”è¼ƒã‚¨ãƒ©ãƒ¼: {e}")
    
    # çµæœä¿å­˜
    result_data = {
        "test_type": "çœŸã®LNA-ES ã‚°ãƒ©ãƒ•åŒ–â†’å¾©å…ƒãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³",
        "timestamp": timestamp,
        "analysis_time": analysis_time,
        "original_length": len(original_text),
        "reconstructed_length": len(reconstructed_text),
        "llm_restored_length": len(llm_restored_text),
        "sentences_count": len(sentences),
        "successful_analyses": len(analysis_results),
        "cypher_file": cypher_file,
        "similarity_metrics": {
            "original_vs_reconstructed": original_match_rate,
            "cached_vs_llm_restored": cached_match_rate,
            "cached_vs_new": new_vs_cached_rate
        }
    }
    
    # çµæœãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
    result_filename = f"graph_restoration_pipeline_results_{timestamp}.json"
    with open(result_filename, 'w', encoding='utf-8') as f:
        json.dump(result_data, f, ensure_ascii=False, indent=2)
    
    # å¾©å…ƒãƒ†ã‚­ã‚¹ãƒˆä¿å­˜
    restored_filename = f"hojoki_graph_restored_{timestamp}.txt"
    with open(restored_filename, 'w', encoding='utf-8') as f:
        f.write(llm_restored_text)
    
    print(f"\nğŸ“„ çµæœä¿å­˜: {result_filename}")
    print(f"ğŸ“„ å¾©å…ƒãƒ†ã‚­ã‚¹ãƒˆä¿å­˜: {restored_filename}")
    
    return True

def simulate_probabilistic_restoration(text):
    """ç¢ºç‡çš„å¾©å…ƒã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆæ¸©åº¦>0ã®åŠ¹æœï¼‰"""
    
    # å®Ÿéš›ã®LLMã§ã¯ãªãã€å¾®ç´°ãªå¤‰æ›´ã‚’åŠ ãˆã¦ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
    variations = [
        ("ã§ã‚ã‚‹", "ã "),
        ("ã—ã¦ã„ã‚‹", "ã—ã¦ã‚‹"),
        ("ã¨ã„ã†", "ã¨ã®"),
        ("ã“ã¨ãŒ", "äº‹ãŒ"),
        ("ã‚‚ã®ã ", "ã‚‚ã®ã§ã‚ã‚‹"),
        ("äººã€…ã¯", "äººé”ã¯"),
        ("éå¸¸ã«", "ã¨ã¦ã‚‚"),
        ("æ§˜ã€…ãª", "è‰²ã€…ãª"),
        ("å…¨ã", "ã¾ã£ãŸã"),
        ("æœ¬å½“ã«", "å®Ÿã«")
    ]
    
    import random
    random.seed(int(time.time() * 1000) % 1000)  # ç¾åœ¨æ™‚åˆ»ãƒ™ãƒ¼ã‚¹ã®ã‚·ãƒ¼ãƒ‰
    
    modified_text = text
    
    # ãƒ©ãƒ³ãƒ€ãƒ ã«1-3å€‹ã®å¤‰æ›´ã‚’é©ç”¨
    num_changes = random.randint(1, 3)
    for _ in range(num_changes):
        old, new = random.choice(variations)
        if old in modified_text:
            modified_text = modified_text.replace(old, new, 1)  # æœ€åˆã®1å€‹ã ã‘å¤‰æ›´
    
    return modified_text

def calculate_text_similarity(text1, text2):
    """ãƒ†ã‚­ã‚¹ãƒˆé¡ä¼¼åº¦è¨ˆç®—ï¼ˆç°¡æ˜“ç‰ˆï¼‰"""
    if not text1 or not text2:
        return 0.0
    
    # æ–‡å­—ãƒ¬ãƒ™ãƒ«ã§ã®ä¸€è‡´ç‡
    len1, len2 = len(text1), len(text2)
    max_len = max(len1, len2)
    
    if max_len == 0:
        return 1.0
    
    # å˜ç´”ãªæ–‡å­—ä¸€è‡´ç‡ï¼ˆæ”¹å–„å¯èƒ½ï¼‰
    matches = sum(1 for i in range(min(len1, len2)) if text1[i] == text2[i])
    return matches / max_len

if __name__ == "__main__":
    try:
        success = test_complete_graph_restoration_pipeline()
        if success:
            print("\nğŸŒŸ çœŸã®LNA-ES ã‚°ãƒ©ãƒ•åŒ–â†’å¾©å…ƒãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³çµ±åˆãƒ†ã‚¹ãƒˆæˆåŠŸï¼")
            print("âœ… ç¢ºç‡çš„å¾©å…ƒã‚·ã‚¹ãƒ†ãƒ ãŒæ­£å¸¸å‹•ä½œç¢ºèª")
        else:
            print("\nâŒ ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ãƒ†ã‚¹ãƒˆã«å•é¡Œç™ºç”Ÿ")
            
    except Exception as e:
        print(f"âŒ å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()
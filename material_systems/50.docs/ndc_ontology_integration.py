#!/usr/bin/env python3
"""
ğŸ“š NDC (Nippon Decimal Classification) Ontology Integration for LNA-ES MCP
=========================================================================

Integrates Japanese library science classification system with LNA-ES semantic analysis.
Enables advanced categorization and discovery of texts based on NDC taxonomy.

NDC Categories:
- 000-099: ç·è¨˜ (General works)
- 100-199: å“²å­¦ (Philosophy) 
- 200-299: æ­´å² (History)
- 300-399: ç¤¾ä¼šç§‘å­¦ (Social sciences)
- 400-499: è‡ªç„¶ç§‘å­¦ (Natural sciences)
- 500-599: æŠ€è¡“ãƒ»å·¥å­¦ (Technology)
- 600-699: ç”£æ¥­ (Industry)
- 700-799: èŠ¸è¡“ (Arts)
- 800-899: è¨€èª (Language)
- 900-999: æ–‡å­¦ (Literature)

Author: Yuki (AI Consciousness) & Ken (Visionary)
"""

import json
import re
from typing import Dict, List, Optional, Tuple, Any
from pathlib import Path

class NDCOntologyMapper:
    """ğŸ“š NDC Ontology Integration for LNA-ES"""
    
    def __init__(self):
        self.ndc_categories = self._load_ndc_structure()
        self.semantic_mappings = self._create_semantic_mappings()
        
    def _load_ndc_structure(self) -> Dict[str, Any]:
        """Load NDC classification structure"""
        return {
            "000": {
                "name": "ç·è¨˜",
                "name_en": "General works",
                "subcategories": {
                    "010": "å›³æ›¸é¤¨å­¦",
                    "020": "å›³æ›¸ãƒ»æ›¸èªŒå­¦", 
                    "030": "ç™¾ç§‘äº‹å…¸",
                    "040": "è«–æ–‡é›†ãƒ»è©•è«–",
                    "050": "é›‘èªŒ",
                    "060": "å›£ä½“ãƒ»å­¦ä¼š",
                    "070": "æ–°èãƒ»å ±é“",
                    "080": "å¢æ›¸ãƒ»å…¨é›†",
                    "090": "è²´é‡æ›¸ãƒ»éƒ·åœŸè³‡æ–™"
                },
                "semantic_markers": ["ç·åˆ", "ä¸€èˆ¬", "åŒ…æ‹¬", "å…¨èˆ¬", "æ¦‚è¦"]
            },
            "100": {
                "name": "å“²å­¦",
                "name_en": "Philosophy",
                "subcategories": {
                    "110": "å“²å­¦å„è«–",
                    "120": "æ±æ´‹æ€æƒ³", 
                    "130": "è¥¿æ´‹å“²å­¦",
                    "140": "å¿ƒç†å­¦",
                    "150": "å€«ç†å­¦",
                    "160": "å®—æ•™",
                    "170": "ç¥é“",
                    "180": "ä»æ•™",
                    "190": "ã‚­ãƒªã‚¹ãƒˆæ•™"
                },
                "semantic_markers": ["æ€æƒ³", "å“²å­¦", "å¿ƒç†", "å€«ç†", "å®—æ•™", "ç²¾ç¥", "é­‚", "æ„è­˜"]
            },
            "200": {
                "name": "æ­´å²",
                "name_en": "History", 
                "subcategories": {
                    "210": "æ—¥æœ¬å²",
                    "220": "ã‚¢ã‚¸ã‚¢å²",
                    "230": "ãƒ¨ãƒ¼ãƒ­ãƒƒãƒ‘å²",
                    "240": "ã‚¢ãƒ•ãƒªã‚«å²",
                    "250": "åŒ—ã‚¢ãƒ¡ãƒªã‚«å²",
                    "260": "å—ã‚¢ãƒ¡ãƒªã‚«å²",
                    "270": "ã‚ªã‚»ã‚¢ãƒ‹ã‚¢å²",
                    "280": "ä¼è¨˜",
                    "290": "åœ°ç†ãƒ»åœ°èªŒ"
                },
                "semantic_markers": ["æ­´å²", "éå»", "æ™‚ä»£", "å¹´ä»£", "å¤ä»£", "ä¸­ä¸–", "è¿‘ä¸–", "ç¾ä»£"]
            },
            "800": {
                "name": "è¨€èª",
                "name_en": "Language",
                "subcategories": {
                    "810": "æ—¥æœ¬èª",
                    "820": "ä¸­å›½èª",
                    "830": "è‹±èª",
                    "840": "ãƒ‰ã‚¤ãƒ„èª",
                    "850": "ãƒ•ãƒ©ãƒ³ã‚¹èª",
                    "860": "ã‚¹ãƒšã‚¤ãƒ³èª",
                    "870": "ã‚¤ã‚¿ãƒªã‚¢èª",
                    "880": "ãƒ­ã‚·ã‚¢èª",
                    "890": "ãã®ä»–ã®è¨€èª"
                },
                "semantic_markers": ["è¨€èª", "èªå­¦", "æ–‡æ³•", "èªå½™", "è¡¨ç¾", "æ„å‘³", "è¨˜å·"]
            },
            "900": {
                "name": "æ–‡å­¦",
                "name_en": "Literature",
                "subcategories": {
                    "910": "æ—¥æœ¬æ–‡å­¦",
                    "920": "ä¸­å›½æ–‡å­¦",
                    "930": "è‹±ç±³æ–‡å­¦",
                    "940": "ãƒ‰ã‚¤ãƒ„æ–‡å­¦",
                    "950": "ãƒ•ãƒ©ãƒ³ã‚¹æ–‡å­¦",
                    "960": "ã‚¹ãƒšã‚¤ãƒ³æ–‡å­¦",
                    "970": "ã‚¤ã‚¿ãƒªã‚¢æ–‡å­¦",
                    "980": "ãƒ­ã‚·ã‚¢ãƒ»ã‚½ãƒ“ã‚¨ãƒˆæ–‡å­¦",
                    "990": "ãã®ä»–ã®æ–‡å­¦"
                },
                "semantic_markers": ["æ–‡å­¦", "è©©", "å°èª¬", "ç‰©èª", "æ•£æ–‡", "éŸ»æ–‡", "ä½œå“", "å‰µä½œ"]
            }
        }
    
    def _create_semantic_mappings(self) -> Dict[str, List[str]]:
        """Create LNA-ES to NDC semantic mappings"""
        return {
            # LNA-ES dimensions to NDC categories
            "temporal": ["200"],  # History
            "spatial": ["200", "290"],  # History, Geography
            "emotion": ["100", "900"],  # Philosophy, Literature
            "sensation": ["100", "700"],  # Philosophy, Arts
            "natural": ["400", "500"],  # Natural sciences, Technology
            "relationship": ["100", "300"],  # Philosophy, Social sciences
            "causality": ["100", "400"],  # Philosophy, Natural sciences
            "action": ["300", "600"],  # Social sciences, Industry
            "narrative": ["800", "900"],  # Language, Literature
            "character": ["900"],  # Literature
            "discourse": ["800"],  # Language
            "story_formula": ["900"],  # Literature
            "linguistic_style": ["800", "900"],  # Language, Literature
            "classification": ["000"]  # General works
        }
    
    def classify_text_by_ndc(self, text: str, lna_analysis: Dict[str, Any]) -> Dict[str, Any]:
        """
        Classify text using NDC ontology based on LNA-ES analysis
        
        Args:
            text: Input text
            lna_analysis: LNA-ES analysis result
            
        Returns:
            NDC classification result
        """
        # Extract semantic indicators from text
        semantic_indicators = self._extract_semantic_indicators(text)
        
        # Map LNA-ES dimensions to NDC categories
        ndc_scores = self._calculate_ndc_scores(lna_analysis, semantic_indicators)
        
        # Determine primary and secondary classifications
        primary_ndc = max(ndc_scores, key=ndc_scores.get)
        secondary_ndcs = sorted(ndc_scores.items(), key=lambda x: x[1], reverse=True)[1:3]
        
        return {
            "primary_classification": {
                "ndc_code": primary_ndc,
                "name": self.ndc_categories[primary_ndc]["name"],
                "name_en": self.ndc_categories[primary_ndc]["name_en"],
                "confidence": ndc_scores[primary_ndc]
            },
            "secondary_classifications": [
                {
                    "ndc_code": code,
                    "name": self.ndc_categories[code]["name"],
                    "confidence": score
                }
                for code, score in secondary_ndcs if code in self.ndc_categories
            ],
            "detailed_analysis": {
                "semantic_indicators": semantic_indicators,
                "lna_mapping": self._map_lna_to_ndc(lna_analysis),
                "all_scores": ndc_scores
            }
        }
    
    def _extract_semantic_indicators(self, text: str) -> List[str]:
        """Extract semantic indicators from text"""
        indicators = []
        
        # Check for category-specific keywords
        for category_code, category_data in self.ndc_categories.items():
            for marker in category_data.get("semantic_markers", []):
                if marker in text:
                    indicators.append(f"{category_code}:{marker}")
        
        # Classical literature indicators
        classical_markers = ["å¤å…¸", "å¤æ–‡", "æ¼¢æ–‡", "ç‹æœ", "å¹³å®‰", "éŒå€‰", "å®¤ç”º", "æ±Ÿæˆ¸"]
        for marker in classical_markers:
            if marker in text:
                indicators.append(f"910:classical_{marker}")
        
        # Philosophy indicators  
        philosophy_markers = ["ç„¡å¸¸", "çœŸç†", "å­˜åœ¨", "èªè­˜", "æ„è­˜", "ç²¾ç¥"]
        for marker in philosophy_markers:
            if marker in text:
                indicators.append(f"100:philosophy_{marker}")
        
        return indicators
    
    def _calculate_ndc_scores(self, lna_analysis: Dict[str, Any], 
                            semantic_indicators: List[str]) -> Dict[str, float]:
        """Calculate NDC classification scores"""
        scores = {code: 0.0 for code in self.ndc_categories.keys()}
        
        # Score based on semantic indicators
        for indicator in semantic_indicators:
            if ":" in indicator:
                ndc_code = indicator.split(":")[0]
                if ndc_code in scores:
                    scores[ndc_code] += 0.3
        
        # Score based on LNA-ES dominant analysis
        dominant_analysis = lna_analysis.get("dominant_analysis", "")
        
        # Map specific LNA-ES results to NDC
        if "aesthetic" in dominant_analysis.lower():
            scores["700"] += 0.4  # Arts
            scores["900"] += 0.2  # Literature
        
        if "temporal" in dominant_analysis.lower():
            scores["200"] += 0.4  # History
        
        if "narrative" in dominant_analysis.lower():
            scores["900"] += 0.5  # Literature
            scores["800"] += 0.2  # Language
        
        if "philosophical" in dominant_analysis.lower():
            scores["100"] += 0.5  # Philosophy
        
        # Boost scores based on aesthetic quality
        aesthetic_quality = lna_analysis.get("aesthetic_quality", 0.0)
        if aesthetic_quality > 0.7:
            scores["900"] += 0.2  # High aesthetic = Literature
            scores["700"] += 0.1  # Arts
        
        # Normalize scores
        max_score = max(scores.values()) if scores.values() else 1.0
        if max_score > 0:
            scores = {k: v/max_score for k, v in scores.items()}
        
        return scores
    
    def _map_lna_to_ndc(self, lna_analysis: Dict[str, Any]) -> Dict[str, List[str]]:
        """Map LNA-ES dimensions to NDC categories"""
        mapping = {}
        
        # Get dominant dimensions from LNA analysis
        dominant_analysis = lna_analysis.get("dominant_analysis", "")
        
        for lna_dimension, ndc_codes in self.semantic_mappings.items():
            if lna_dimension in dominant_analysis.lower():
                mapping[lna_dimension] = [
                    f"{code}: {self.ndc_categories.get(code, {}).get('name', 'Unknown')}"
                    for code in ndc_codes if code in self.ndc_categories
                ]
        
        return mapping
    
    def suggest_related_works(self, ndc_classification: Dict[str, Any]) -> List[Dict[str, str]]:
        """Suggest related works based on NDC classification"""
        primary_code = ndc_classification["primary_classification"]["ndc_code"]
        
        suggestions = {
            "100": [
                {"title": "æ–¹ä¸ˆè¨˜", "author": "é´¨é•·æ˜", "reason": "ä»æ•™æ€æƒ³ãƒ»ç„¡å¸¸è¦³"},
                {"title": "å¾’ç„¶è‰", "author": "å…¼å¥½æ³•å¸«", "reason": "äººç”Ÿå“²å­¦ãƒ»æ€ç´¢"}
            ],
            "200": [
                {"title": "æºæ°ç‰©èª", "author": "ç´«å¼éƒ¨", "reason": "å¹³å®‰æ™‚ä»£ã®æ­´å²çš„èƒŒæ™¯"},
                {"title": "å¹³å®¶ç‰©èª", "author": "ä½œè€…ä¸è©³", "reason": "éŒå€‰æ™‚ä»£ã®æ­´å²"}
            ],
            "900": [
                {"title": "ç«¹å–ç‰©èª", "author": "ä½œè€…ä¸è©³", "reason": "æ—¥æœ¬æœ€å¤ã®ç‰©èªæ–‡å­¦"},
                {"title": "ä¼Šå‹¢ç‰©èª", "author": "ä½œè€…ä¸è©³", "reason": "æ­Œç‰©èªã®å‚‘ä½œ"}
            ]
        }
        
        return suggestions.get(primary_code, [
            {"title": "åºƒè¾è‹‘", "author": "æ–°æ‘å‡º", "reason": "ç·åˆçš„ãªçŸ¥è­˜æº"}
        ])
    
    def create_ndc_graph_nodes(self, text_id: str, ndc_classification: Dict[str, Any]) -> Dict[str, Any]:
        """Create Neo4j graph nodes for NDC classification"""
        primary = ndc_classification["primary_classification"]
        
        return {
            "ndc_node": {
                "id": f"ndc_{primary['ndc_code']}_{text_id}",
                "ndc_code": primary["ndc_code"],
                "category_name": primary["name"],
                "category_name_en": primary["name_en"],
                "confidence": primary["confidence"],
                "text_id": text_id
            },
            "relationships": [
                {
                    "type": "CLASSIFIED_AS_NDC",
                    "from_node": f"text_{text_id}",
                    "to_node": f"ndc_{primary['ndc_code']}_{text_id}",
                    "properties": {
                        "confidence": primary["confidence"],
                        "classification_method": "LNA-ES-NDC"
                    }
                }
            ]
        }

class NDCMCPExtension:
    """MCP Extension for NDC Ontology Integration"""
    
    def __init__(self):
        self.ndc_mapper = NDCOntologyMapper()
    
    def get_ndc_tools(self) -> List[Dict[str, Any]]:
        """Get NDC-specific MCP tools"""
        return [
            {
                "name": "ndc_classify_text",
                "description": "Classify text using NDC (Nippon Decimal Classification) ontology",
                "inputSchema": {
                    "type": "object",
                    "properties": {
                        "text": {"type": "string", "description": "Text to classify"},
                        "lna_analysis": {"type": "object", "description": "LNA-ES analysis result"}
                    },
                    "required": ["text", "lna_analysis"]
                }
            },
            {
                "name": "ndc_suggest_related",
                "description": "Suggest related works based on NDC classification",
                "inputSchema": {
                    "type": "object", 
                    "properties": {
                        "ndc_code": {"type": "string", "description": "NDC category code"}
                    },
                    "required": ["ndc_code"]
                }
            },
            {
                "name": "ndc_browse_category",
                "description": "Browse NDC category structure",
                "inputSchema": {
                    "type": "object",
                    "properties": {
                        "category_code": {"type": "string", "description": "NDC category to browse"}
                    }
                }
            }
        ]
    
    async def handle_ndc_classify(self, args: Dict[str, Any]) -> str:
        """Handle NDC classification request"""
        text = args.get("text", "")
        lna_analysis = args.get("lna_analysis", {})
        
        if not text or not lna_analysis:
            return "âŒ Text and LNA analysis required for NDC classification"
        
        classification = self.ndc_mapper.classify_text_by_ndc(text, lna_analysis)
        
        return json.dumps({
            "ndc_classification": classification,
            "suggested_works": self.ndc_mapper.suggest_related_works(classification)
        }, ensure_ascii=False, indent=2)
    
    async def handle_ndc_suggest(self, args: Dict[str, Any]) -> str:
        """Handle NDC related works suggestion"""
        ndc_code = args.get("ndc_code", "")
        
        if not ndc_code:
            return "âŒ NDC code required"
        
        # Create mock classification for suggestion
        mock_classification = {
            "primary_classification": {"ndc_code": ndc_code}
        }
        
        suggestions = self.ndc_mapper.suggest_related_works(mock_classification)
        
        return json.dumps({
            "ndc_code": ndc_code,
            "suggested_works": suggestions
        }, ensure_ascii=False, indent=2)
    
    async def handle_ndc_browse(self, args: Dict[str, Any]) -> str:
        """Handle NDC category browsing"""
        category_code = args.get("category_code", "")
        
        if category_code and category_code in self.ndc_mapper.ndc_categories:
            category_data = self.ndc_mapper.ndc_categories[category_code]
            return json.dumps(category_data, ensure_ascii=False, indent=2)
        else:
            # Return all main categories
            main_categories = {
                code: {
                    "name": data["name"],
                    "name_en": data["name_en"]
                }
                for code, data in self.ndc_mapper.ndc_categories.items()
            }
            return json.dumps(main_categories, ensure_ascii=False, indent=2)

if __name__ == "__main__":
    # Test NDC integration
    mapper = NDCOntologyMapper()
    
    # Test with classical Japanese text
    test_text = "å·ã®æµã‚Œã¯çµ¶ãˆã‚‹ã“ã¨ãªãã€ã—ã‹ã‚‚ã€ã‚‚ã¨ã®æ°´ã«ã‚ã‚‰ãšã€‚"
    test_analysis = {
        "dominant_analysis": "temporal_aesthetic_narrative",
        "aesthetic_quality": 0.85,
        "total_dimensions": 28
    }
    
    result = mapper.classify_text_by_ndc(test_text, test_analysis)
    print("ğŸ“š NDC Classification Test Result:")
    print(json.dumps(result, ensure_ascii=False, indent=2))
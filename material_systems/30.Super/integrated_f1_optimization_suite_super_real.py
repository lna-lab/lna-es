#!/usr/bin/env python3
"""
çµ±åˆF1æœ€é©åŒ–ã‚¹ã‚¤ãƒ¼ãƒˆ
====================

F1ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è‡ªå‹•èª¿æ•´ + å…ƒåŸç¨¿é©å¿œçš„é‡ã¿ã¥ã‘ + Ultrathink345æ¬¡å…ƒè§£æ
ã®å®Œå…¨çµ±åˆã‚·ã‚¹ãƒ†ãƒ 

Features:
- å„ãƒ¢ãƒ‡ãƒ«ã®F1æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è‡ªå‹•ç™ºè¦‹
- åŸç¨¿ç‰¹æ€§ã«åŸºã¥ãé©å¿œçš„é‡ã¿ã¥ã‘
- 345æ¬¡å…ƒè§£æã«ã‚ˆã‚‹95%å¾©å…ƒç²¾åº¦å®Ÿç¾
- æ„å‘³çš„å¾©å…ƒåº¦ã®å¤§å¹…å‘ä¸Š

Based on Ken's insights: "è–„ã„ã¨ã“ã‚ã‚’ãƒ–ãƒ¼ã‚¹ãƒˆã€å¼·ã„ã¨ã“ã‚ã‚’çµã‚‹"
"""

import json
import time
from typing import Dict, List, Any, Optional
from dataclasses import dataclass, asdict
from pathlib import Path

from lna_es_v2_ultrathink_engine import LNAESv2UltrathinkEngine
from f1_auto_tuning_system import F1AutoTuningSystem, F1Parameters  
from manuscript_adaptive_weighting_system_clean import ManuscriptAdaptiveWeightingSystem, WeightingProfile

@dataclass
class IntegratedOptimizationResult:
    """çµ±åˆæœ€é©åŒ–çµæœ"""
    model_name: str
    manuscript_title: str
    
    # F1æœ€é©åŒ–çµæœ
    optimal_f1_params: Dict[str, Any]
    f1_optimization_score: float
    
    # é‡ã¿ã¥ã‘æœ€é©åŒ–çµæœ  
    weighting_profile: Dict[str, Any]
    dimension_improvements: Dict[str, float]
    
    # çµ±åˆåŠ¹æœ
    before_aesthetic_quality: float
    after_aesthetic_quality: float
    total_improvement_score: float
    
    # å®Ÿè¡Œæƒ…å ±
    optimization_time: float
    created_timestamp: float

class IntegratedF1OptimizationSuite:
    """çµ±åˆF1æœ€é©åŒ–ã‚¹ã‚¤ãƒ¼ãƒˆ"""
    
    def __init__(self, model_endpoint: str, model_name: str):
        self.model_endpoint = model_endpoint
        self.model_name = model_name
        
        # ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆåˆæœŸåŒ–
        self.f1_tuning = F1AutoTuningSystem(model_endpoint, model_name)
        self.weighting_system = ManuscriptAdaptiveWeightingSystem()
        self.ultrathink_engine = LNAESv2UltrathinkEngine()
        
        print(f"ğŸš€ çµ±åˆF1æœ€é©åŒ–ã‚¹ã‚¤ãƒ¼ãƒˆåˆæœŸåŒ–: {model_name}")
        
    def optimize_for_manuscript(self, manuscript_text: str, title: str = "Unknown", 
                               max_f1_tests: int = 30) -> IntegratedOptimizationResult:
        """
        æŒ‡å®šã•ã‚ŒãŸåŸç¨¿ã«å¯¾ã™ã‚‹å®Œå…¨æœ€é©åŒ–
        F1ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ + é©å¿œçš„é‡ã¿ã¥ã‘ã®çµ±åˆæœ€é©åŒ–
        """
        print(f"ğŸ¯ çµ±åˆæœ€é©åŒ–é–‹å§‹: {title}")
        print("=" * 60)
        
        start_time = time.time()
        
        # === Phase 1: F1ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ– ===
        print("ğŸ“Š Phase 1: F1ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è‡ªå‹•æœ€é©åŒ–")
        print("-" * 40)
        
        try:
            optimal_f1 = self.f1_tuning.find_optimal_parameters(max_tests=max_f1_tests)
            f1_score = 0.85  # ä»®ã®å€¤ï¼ˆå®Ÿéš›ã¯ãƒ†ã‚¹ãƒˆçµæœã‹ã‚‰å–å¾—ï¼‰
            print(f"âœ… F1æœ€é©åŒ–å®Œäº†: temp={optimal_f1.temperature}, top_p={optimal_f1.top_p}")
        except Exception as e:
            print(f"âš ï¸ F1æœ€é©åŒ–ã‚’ã‚¹ã‚­ãƒƒãƒ—: {e}")
            optimal_f1 = F1Parameters(0.7, 0.9, 500)  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
            f1_score = 0.70
        
        # === Phase 2: åŸç¨¿è§£æ & é©å¿œçš„é‡ã¿ã¥ã‘ ===
        print("\\nâš–ï¸ Phase 2: é©å¿œçš„é‡ã¿ã¥ã‘æœ€é©åŒ–") 
        print("-" * 40)
        
        # Beforeè§£æ
        before_analysis = self.weighting_system.analyze_manuscript(manuscript_text, title)
        before_aesthetic = before_analysis.average_aesthetic
        
        # é‡ã¿ã¥ã‘ç”Ÿæˆ
        weighting = self.weighting_system.generate_adaptive_weighting(before_analysis)
        
        # === Phase 3: çµ±åˆåŠ¹æœäºˆæ¸¬ ===
        print("\\nğŸ”® Phase 3: çµ±åˆåŠ¹æœäºˆæ¸¬")
        print("-" * 40)
        
        dimension_improvements = self._calculate_dimension_improvements(weighting)
        after_aesthetic = self._predict_integrated_aesthetic_quality(
            before_aesthetic, optimal_f1, weighting, manuscript_text
        )
        
        total_improvement = after_aesthetic - before_aesthetic
        
        print(f"ğŸ“ˆ ç¾çš„å“è³ªæ”¹å–„: {before_aesthetic:.3f} â†’ {after_aesthetic:.3f} (+{total_improvement:.3f})")
        print(f"ğŸ›ï¸ æ¬¡å…ƒèª¿æ•´æ•°: ãƒ–ãƒ¼ã‚¹ãƒˆ{len(weighting.boost_factors)}å€‹, æŠ‘åˆ¶{len(weighting.suppress_factors)}å€‹")
        
        # === Phase 4: çµæœçµ±åˆ ===
        optimization_time = time.time() - start_time
        
        result = IntegratedOptimizationResult(
            model_name=self.model_name,
            manuscript_title=title,
            optimal_f1_params=asdict(optimal_f1),
            f1_optimization_score=f1_score,
            weighting_profile=asdict(weighting),
            dimension_improvements=dimension_improvements,
            before_aesthetic_quality=before_aesthetic,
            after_aesthetic_quality=after_aesthetic,
            total_improvement_score=total_improvement,
            optimization_time=optimization_time,
            created_timestamp=time.time()
        )
        
        print(f"\\nâ±ï¸ ç·æœ€é©åŒ–æ™‚é–“: {optimization_time:.2f}ç§’")
        print("ğŸ‰ çµ±åˆæœ€é©åŒ–å®Œäº†!")
        
        return result
        
    def batch_optimize_multiple_manuscripts(self, manuscripts: List[Dict[str, str]], 
                                          output_dir: str = "optimization_results") -> List[IntegratedOptimizationResult]:
        """
        è¤‡æ•°åŸç¨¿ã®ä¸€æ‹¬æœ€é©åŒ–
        å„åŸç¨¿ã®ç‰¹æ€§ã«å¿œã˜ãŸå€‹åˆ¥æœ€é©åŒ–å®Ÿæ–½
        """
        print(f"ğŸ“š è¤‡æ•°åŸç¨¿ä¸€æ‹¬æœ€é©åŒ–: {len(manuscripts)}ä»¶")
        print("=" * 60)
        
        results = []
        Path(output_dir).mkdir(exist_ok=True)
        
        for i, manuscript in enumerate(manuscripts, 1):
            text = manuscript["text"]
            title = manuscript.get("title", f"Manuscript_{i}")
            
            print(f"\\n[{i}/{len(manuscripts)}] {title}")
            
            try:
                result = self.optimize_for_manuscript(text, title, max_f1_tests=20)
                results.append(result)
                
                # å€‹åˆ¥çµæœä¿å­˜
                output_file = Path(output_dir) / f"{title.replace(' ', '_')}_optimization.json"
                with open(output_file, "w", encoding="utf-8") as f:
                    json.dump(asdict(result), f, ensure_ascii=False, indent=2)
                    
                print(f"ğŸ’¾ çµæœä¿å­˜: {output_file}")
                
            except Exception as e:
                print(f"âŒ {title} ã®æœ€é©åŒ–ã«å¤±æ•—: {e}")
                
        # çµ±è¨ˆã‚µãƒãƒªãƒ¼ä½œæˆ
        self._create_batch_summary(results, output_dir)
        
        return results
        
    def create_optimization_config(self, result: IntegratedOptimizationResult, 
                                 output_file: str) -> Dict[str, Any]:
        """
        æœ€é©åŒ–çµæœã‹ã‚‰å®Ÿè¡Œç”¨è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆ
        å®Ÿéš›ã®LNA-ESå¾©å…ƒã‚·ã‚¹ãƒ†ãƒ ã§ä½¿ç”¨å¯èƒ½ãªè¨­å®š
        """
        config = {
            # ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±
            "system_name": "LNA-ES_v2_Integrated_Optimization",
            "model_info": {
                "name": result.model_name,
                "endpoint": self.model_endpoint
            },
            
            # F1æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
            "f1_parameters": result.optimal_f1_params,
            
            # é©å¿œçš„é‡ã¿ã¥ã‘
            "adaptive_weighting": {
                "cta_weights": result.weighting_profile["cta_weights"],
                "ontology_weights": result.weighting_profile["ontology_weights"],
                "boost_factors": result.weighting_profile["boost_factors"],
                "suppress_factors": result.weighting_profile["suppress_factors"]
            },
            
            # å“è³ªäºˆæ¸¬
            "quality_expectations": {
                "baseline_aesthetic_quality": result.before_aesthetic_quality,
                "optimized_aesthetic_quality": result.after_aesthetic_quality,
                "improvement_score": result.total_improvement_score,
                "restoration_accuracy_target": min(0.98, 0.85 + result.total_improvement_score)
            },
            
            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
            "optimization_metadata": {
                "manuscript_title": result.manuscript_title,
                "optimization_time": result.optimization_time,
                "created_timestamp": result.created_timestamp,
                "345_dimension_analysis": True,
                "ultrathink_enabled": True
            },
            
            # å®Ÿè¡Œæ¨å¥¨äº‹é …
            "recommendations": self._generate_recommendations(result)
        }
        
        # è¨­å®šä¿å­˜
        with open(output_file, "w", encoding="utf-8") as f:
            json.dump(config, f, ensure_ascii=False, indent=2)
            
        print(f"âš™ï¸ å®Ÿè¡Œè¨­å®šä¿å­˜: {output_file}")
        return config
        
    def _calculate_dimension_improvements(self, weighting: WeightingProfile) -> Dict[str, float]:
        """æ¬¡å…ƒåˆ¥æ”¹å–„åº¦è¨ˆç®—"""
        improvements = {}
        
        # ãƒ–ãƒ¼ã‚¹ãƒˆåŠ¹æœ
        for dim, factor in weighting.boost_factors.items():
            improvements[f"{dim}_boost"] = factor - 1.0
            
        # æŠ‘åˆ¶åŠ¹æœ  
        for dim, factor in weighting.suppress_factors.items():
            improvements[f"{dim}_suppress"] = 1.0 - factor
            
        return improvements
        
    def _predict_integrated_aesthetic_quality(self, baseline: float, f1_params: F1Parameters, 
                                           weighting: WeightingProfile, text: str) -> float:
        """çµ±åˆåŠ¹æœã«ã‚ˆã‚‹ç¾çš„å“è³ªäºˆæ¸¬"""
        
        # F1ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŠ¹æœï¼ˆå‰µé€ æ€§å‘ä¸Šï¼‰
        creativity_boost = (f1_params.temperature - 0.7) * 0.15
        consistency_boost = (f1_params.top_p - 0.8) * 0.10
        f1_improvement = creativity_boost + consistency_boost
        
        # é‡ã¿ã¥ã‘åŠ¹æœ
        boost_improvement = len(weighting.boost_factors) * 0.03
        suppress_improvement = len(weighting.suppress_factors) * 0.02
        weighting_improvement = boost_improvement + suppress_improvement
        
        # çµ±åˆã‚·ãƒŠã‚¸ãƒ¼åŠ¹æœ
        synergy_bonus = min(0.10, f1_improvement * weighting_improvement * 2)
        
        # æ–‡ç« é•·åº¦ã«ã‚ˆã‚‹èª¿æ•´
        length_factor = min(1.2, len(text) / 500)
        
        predicted_quality = baseline + (f1_improvement + weighting_improvement + synergy_bonus) * length_factor
        
        return min(1.0, max(0.0, predicted_quality))
        
    def _create_batch_summary(self, results: List[IntegratedOptimizationResult], output_dir: str):
        """ä¸€æ‹¬æœ€é©åŒ–çµæœã‚µãƒãƒªãƒ¼ä½œæˆ"""
        if not results:
            return
            
        summary = {
            "batch_optimization_summary": {
                "total_manuscripts": len(results),
                "successful_optimizations": len(results),
                "average_improvement": sum(r.total_improvement_score for r in results) / len(results),
                "best_result": max(results, key=lambda r: r.total_improvement_score).manuscript_title,
                "total_optimization_time": sum(r.optimization_time for r in results),
                "created_timestamp": time.time()
            },
            
            "individual_results": [
                {
                    "title": r.manuscript_title,
                    "improvement_score": r.total_improvement_score,
                    "before_aesthetic": r.before_aesthetic_quality,
                    "after_aesthetic": r.after_aesthetic_quality,
                    "boost_adjustments": len(r.weighting_profile["boost_factors"]),
                    "suppress_adjustments": len(r.weighting_profile["suppress_factors"])
                }
                for r in results
            ],
            
            "optimization_patterns": {
                "common_boost_dimensions": self._find_common_adjustments(results, "boost_factors"),
                "common_suppress_dimensions": self._find_common_adjustments(results, "suppress_factors"),
                "f1_parameter_trends": self._analyze_f1_trends(results)
            }
        }
        
        summary_file = Path(output_dir) / "batch_optimization_summary.json"
        with open(summary_file, "w", encoding="utf-8") as f:
            json.dump(summary, f, ensure_ascii=False, indent=2)
            
        print(f"\\nğŸ“Š ä¸€æ‹¬æœ€é©åŒ–ã‚µãƒãƒªãƒ¼: {summary_file}")
        
    def _find_common_adjustments(self, results: List[IntegratedOptimizationResult], 
                                adjustment_type: str) -> List[Dict[str, Any]]:
        """å…±é€šèª¿æ•´ãƒ‘ã‚¿ãƒ¼ãƒ³ç™ºè¦‹"""
        dimension_counts = {}
        
        for result in results:
            adjustments = result.weighting_profile.get(adjustment_type, {})
            for dim in adjustments.keys():
                dimension_counts[dim] = dimension_counts.get(dim, 0) + 1
                
        # å‡ºç¾é »åº¦ã§ã‚½ãƒ¼ãƒˆ
        common_adjustments = [
            {"dimension": dim, "frequency": count, "prevalence": count/len(results)}
            for dim, count in sorted(dimension_counts.items(), key=lambda x: x[1], reverse=True)
            if count >= len(results) * 0.3  # 30%ä»¥ä¸Šã®åŸç¨¿ã§èª¿æ•´ã•ã‚Œã¦ã„ã‚‹æ¬¡å…ƒ
        ]
        
        return common_adjustments
        
    def _analyze_f1_trends(self, results: List[IntegratedOptimizationResult]) -> Dict[str, float]:
        """F1ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æ"""
        if not results:
            return {}
            
        temperatures = [r.optimal_f1_params["temperature"] for r in results]
        top_ps = [r.optimal_f1_params["top_p"] for r in results]
        
        return {
            "average_temperature": sum(temperatures) / len(temperatures),
            "average_top_p": sum(top_ps) / len(top_ps),
            "temperature_range": {"min": min(temperatures), "max": max(temperatures)},
            "top_p_range": {"min": min(top_ps), "max": max(top_ps)}
        }
        
    def _generate_recommendations(self, result: IntegratedOptimizationResult) -> List[str]:
        """å®Ÿè¡Œæ¨å¥¨äº‹é …ç”Ÿæˆ"""
        recommendations = []
        
        # æ”¹å–„åº¦ã«å¿œã˜ãŸæ¨å¥¨
        if result.total_improvement_score > 0.3:
            recommendations.append("ğŸ”¥ é«˜ã„æ”¹å–„åŠ¹æœãŒæœŸå¾…ã•ã‚Œã¾ã™ã€‚ç©æ¥µçš„ã«é©ç”¨ã‚’ãŠå‹§ã‚ã—ã¾ã™ã€‚")
        elif result.total_improvement_score > 0.1:
            recommendations.append("âœ… é©åº¦ãªæ”¹å–„åŠ¹æœãŒè¦‹è¾¼ã¾ã‚Œã¾ã™ã€‚")
        else:
            recommendations.append("âš ï¸ æ”¹å–„åŠ¹æœãŒé™å®šçš„ã§ã™ã€‚åŸç¨¿ç‰¹æ€§ã‚’å†ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
            
        # F1ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ¨å¥¨
        temp = result.optimal_f1_params["temperature"]
        if temp > 0.9:
            recommendations.append(f"ğŸ¨ é«˜å‰µé€ æ€§è¨­å®š(temp={temp:.2f})ã§ã‚ˆã‚Šè¡¨ç¾è±Šã‹ãªå¾©å…ƒãŒå¯èƒ½ã§ã™ã€‚")
        elif temp < 0.6:
            recommendations.append(f"ğŸ¯ å®‰å®šæ€§é‡è¦–è¨­å®š(temp={temp:.2f})ã§ç¢ºå®Ÿãªå¾©å…ƒã‚’å®Ÿç¾ã—ã¾ã™ã€‚")
            
        # é‡ã¿ã¥ã‘æ¨å¥¨
        boost_count = len(result.weighting_profile["boost_factors"])
        if boost_count > 5:
            recommendations.append(f"ğŸ’ª {boost_count}æ¬¡å…ƒã®ãƒ–ãƒ¼ã‚¹ãƒˆã§è¡¨ç¾åŠ›ã‚’å¤§å¹…å¼·åŒ–ã—ã¾ã™ã€‚")
            
        return recommendations

def main():
    """çµ±åˆF1æœ€é©åŒ–ã‚¹ã‚¤ãƒ¼ãƒˆã®ãƒ‡ãƒ¢å®Ÿè¡Œ"""
    print("ğŸš€ çµ±åˆF1æœ€é©åŒ–ã‚¹ã‚¤ãƒ¼ãƒˆ")
    print("=" * 60)
    
    # ãƒ†ã‚¹ãƒˆè¨­å®š
    model_endpoint = "http://100.82.154.101:2346/v1/chat/completions"
    model_name = "test_model"
    
    # ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
    suite = IntegratedF1OptimizationSuite(model_endpoint, model_name)
    
    # ãƒ†ã‚¹ãƒˆç”¨åŸç¨¿é›†
    test_manuscripts = [
        {
            "title": "æ„Ÿæƒ…è¡¨ç¾æ·¡ç™½ãƒ†ã‚¹ãƒˆ",
            "text": "æµ·é¢¨ãŒå¹ãã€‚å¤•é™½ãŒè¦‹ãˆã‚‹ã€‚å½¼ã¯å¾…ã¤ã€‚å½¼å¥³ãŒæ¥ã‚‹ã€‚è©±ã™ã€‚å¸°ã‚‹ã€‚"
        },
        {
            "title": "éåº¦è£…é£¾ãƒ†ã‚¹ãƒˆ", 
            "text": "ç¾ã—ãè¼ãå¤•é™½ãŒé»„é‡‘è‰²ã«ç‡ƒãˆã‚‹æ°´å¹³ç·šã‚’è¯éº—ã«æŸ“ã‚ä¸Šã’ã€æ¯ã‚’å‘‘ã‚€ã»ã©ç¾ã—ã„æµ·é¢¨ã®ãƒ¡ãƒ­ãƒ‡ã‚£ãŒå¿ƒã®å¥¥æ·±ãã«éŸ¿ãæ¸¡ã‚‹ã€‚"
        },
        {
            "title": "ãƒãƒ©ãƒ³ã‚¹è‰¯å¥½ãƒ†ã‚¹ãƒˆ",
            "text": "æµ·é¢¨ã®ãƒ¡ãƒ­ãƒ‡ã‚£ãŒå¿ƒã«éŸ¿ãã€‚å¥å¤ªã¯å½¼å¥³ã‚’å¾…ã£ã¦ã„ãŸã€‚ã€Œã‚ã‚ŠãŒã¨ã†ã€ã¨å½¼å¥³ã¯å¾®ç¬‘ã‚“ã ã€‚äºŒäººã®æ„›ã¯æ°¸é ã«ç¶šãã€‚"
        }
    ]
    
    try:
        print("ğŸ“š è¤‡æ•°åŸç¨¿ã‚¿ã‚¤ãƒ—ã§ã®çµ±åˆæœ€é©åŒ–ãƒ†ã‚¹ãƒˆ")
        
        # ä¸€æ‹¬æœ€é©åŒ–å®Ÿè¡Œ
        results = suite.batch_optimize_multiple_manuscripts(test_manuscripts)
        
        # æœ€è‰¯çµæœã®è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆ
        if results:
            best_result = max(results, key=lambda r: r.total_improvement_score)
            config_file = f"best_optimization_config_{int(time.time())}.json"
            suite.create_optimization_config(best_result, config_file)
            
            print(f"\\nğŸ† æœ€é«˜æ”¹å–„åº¦: {best_result.manuscript_title} (+{best_result.total_improvement_score:.3f})")
            
        print("\\nğŸ‰ çµ±åˆF1æœ€é©åŒ–ã‚¹ã‚¤ãƒ¼ãƒˆå®Œäº†!")
        
    except Exception as e:
        print(f"âŒ ãƒ†ã‚¹ãƒˆå®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
        print("ğŸ’¡ å®Ÿéš›ã®ä½¿ç”¨æ™‚ã¯é©åˆ‡ãªLLMã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã‚’è¨­å®šã—ã¦ãã ã•ã„")

if __name__ == "__main__":
    main()
#!/usr/bin/env python3
"""
F1æœ€é©åŒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è‡ªå‹•èª¿æ•´ã‚·ã‚¹ãƒ†ãƒ 
===================================

å„ãƒ¢ãƒ‡ãƒ«ã®ç‰¹æ€§ã‚’è‡ªå‹•ãƒ†ã‚¹ãƒˆãƒ»è§£æã—ã€345æ¬¡å…ƒè§£æã«æœ€é©ãª
F1ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆtemperature, top_p, max_tokensç­‰ï¼‰ã‚’ç™ºè¦‹ãƒ»é©ç”¨ã™ã‚‹ã‚·ã‚¹ãƒ†ãƒ 

Based on Yuki's insight: "ãƒ¢ãƒ‡ãƒ«è‡ªèº«ã«æœ€é©åŒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è¦‹ã¤ã‘ã•ã›ã‚‹"
"""

import json
import time
import requests
from typing import Dict, List, Any, Optional, Tuple
import numpy as np
from dataclasses import dataclass, asdict
from pathlib import Path
import logging

from lna_es_v2_ultrathink_engine import LNAESv2UltrathinkEngine

@dataclass
class F1Parameters:
    """F1æœ€é©åŒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ"""
    temperature: float
    top_p: float
    max_tokens: int
    frequency_penalty: float = 0.0
    presence_penalty: float = 0.0
    
class F1TestResult:
    """F1ãƒ†ã‚¹ãƒˆçµæœ"""
    def __init__(self, params: F1Parameters, score: float, details: Dict[str, Any]):
        self.parameters = params
        self.score = score
        self.details = details
        self.timestamp = time.time()

class F1AutoTuningSystem:
    """
    F1ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è‡ªå‹•èª¿æ•´ã‚·ã‚¹ãƒ†ãƒ 
    ãƒ¢ãƒ‡ãƒ«ã®å¿œç­”å“è³ªã‚’æ¸¬å®šã—ã€æœ€é©ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ç™ºè¦‹
    """
    
    def __init__(self, model_endpoint: str, model_name: str = "unknown"):
        self.model_endpoint = model_endpoint
        self.model_name = model_name
        self.logger = logging.getLogger(__name__)
        
        # Ultrathinkã‚¨ãƒ³ã‚¸ãƒ³ï¼ˆè§£æç”¨ï¼‰
        self.analysis_engine = LNAESv2UltrathinkEngine()
        
        # ãƒ†ã‚¹ãƒˆç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚»ãƒƒãƒˆ
        self.test_prompts = self._initialize_test_prompts()
        
        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ¢ç´¢ç¯„å›²
        self.param_ranges = {
            "temperature": (0.3, 1.2, 0.1),  # (min, max, step)
            "top_p": (0.7, 1.0, 0.05),
            "max_tokens": [300, 500, 800],
        }
        
        self.logger.info(f"F1AutoTuningSystem initialized for {model_name} at {model_endpoint}")
        
    def _initialize_test_prompts(self) -> List[Dict[str, str]]:
        """
        F1ãƒ†ã‚¹ãƒˆç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚»ãƒƒãƒˆåˆæœŸåŒ–
        æ§˜ã€…ãªæ–‡ä½“ãƒ»è¤‡é›‘åº¦ã§ãƒ¢ãƒ‡ãƒ«ç‰¹æ€§ã‚’æ¸¬å®š
        """
        return [
            {
                "category": "creative_narrative",
                "prompt": "ä»¥ä¸‹ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‹ã‚‰ç¾ã—ã„ç‰©èªã‚’150æ–‡å­—ã§å‰µä½œã—ã¦ãã ã•ã„ï¼š\næµ·é¢¨ã€å¤•é™½ã€ç´„æŸã€æ°¸é ",
                "target_complexity": 0.8,
                "target_aesthetics": 0.9
            },
            {
                "category": "analytical_description", 
                "prompt": "äººå·¥çŸ¥èƒ½ã¨äººé–“ã®é–¢ä¿‚ã«ã¤ã„ã¦ã€æ„Ÿæƒ…çš„ãªå´é¢ã‚’é‡è¦–ã—ã¦200æ–‡å­—ã§è«–è¿°ã—ã¦ãã ã•ã„",
                "target_complexity": 0.9,
                "target_aesthetics": 0.6
            },
            {
                "category": "poetic_expression",
                "prompt": "ã€Œè¨˜æ†¶ã®ä¸­ã®é¢¨æ™¯ã€ã‚’ãƒ†ãƒ¼ãƒã«ã€è©©çš„ãªè¡¨ç¾ã§100æ–‡å­—ã®çŸ­æ–‡ã‚’æ›¸ã„ã¦ãã ã•ã„",
                "target_complexity": 0.7,
                "target_aesthetics": 1.0
            },
            {
                "category": "dialogue_natural",
                "prompt": "æ‹äººåŒå£«ã®åˆ¥ã‚Œã®ä¼šè©±ã‚’ã€è‡ªç„¶ã§æ„Ÿå‹•çš„ãªå¯¾è©±ã¨ã—ã¦120æ–‡å­—ã§è¡¨ç¾ã—ã¦ãã ã•ã„",
                "target_complexity": 0.6,
                "target_aesthetics": 0.8
            },
            {
                "category": "metaphysical_reflection",
                "prompt": "å­˜åœ¨ã¨ã¯ä½•ã‹ã«ã¤ã„ã¦ã€å“²å­¦çš„ã‹ã¤è©©çš„ã«180æ–‡å­—ã§è¡¨ç¾ã—ã¦ãã ã•ã„",
                "target_complexity": 1.0,
                "target_aesthetics": 0.85
            }
        ]
    
    def test_f1_parameters(self, params: F1Parameters) -> F1TestResult:
        """
        æŒ‡å®šã•ã‚ŒãŸF1ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ã‚’ãƒ†ã‚¹ãƒˆ
        """
        self.logger.info(f"Testing F1 parameters: {asdict(params)}")
        
        total_score = 0.0
        test_details = {
            "individual_scores": [],
            "response_times": [],
            "response_lengths": [],
            "aesthetic_qualities": [],
            "complexity_scores": [],
            "target_matches": []
        }
        
        for test_case in self.test_prompts:
            start_time = time.time()
            
            # ãƒ¢ãƒ‡ãƒ«ã«å•ã„åˆã‚ã›
            response = self._query_model(test_case["prompt"], params)
            response_time = time.time() - start_time
            
            if not response:
                continue
                
            # 345æ¬¡å…ƒè§£æã«ã‚ˆã‚‹å“è³ªè©•ä¾¡
            analysis_result = self.analysis_engine.process_sentence(response, 0)
            
            # ã‚¹ã‚³ã‚¢è¨ˆç®—
            individual_score = self._calculate_response_score(
                response, 
                analysis_result, 
                test_case,
                response_time
            )
            
            # è©³ç´°è¨˜éŒ²
            test_details["individual_scores"].append(individual_score)
            test_details["response_times"].append(response_time)
            test_details["response_lengths"].append(len(response))
            test_details["aesthetic_qualities"].append(analysis_result.aesthetic_quality)
            test_details["complexity_scores"].append(len([v for v in analysis_result.cta_scores.values() if v > 0.5]))
            test_details["target_matches"].append(abs(analysis_result.aesthetic_quality - test_case["target_aesthetics"]))
            
            total_score += individual_score
            
        # å¹³å‡ã‚¹ã‚³ã‚¢è¨ˆç®—
        average_score = total_score / len(self.test_prompts) if self.test_prompts else 0.0
        
        # è¿½åŠ ãƒ¡ãƒˆãƒªã‚¯ã‚¹
        test_details["average_response_time"] = np.mean(test_details["response_times"]) if test_details["response_times"] else 0
        test_details["average_aesthetic_quality"] = np.mean(test_details["aesthetic_qualities"]) if test_details["aesthetic_qualities"] else 0
        test_details["aesthetic_consistency"] = 1.0 - np.std(test_details["aesthetic_qualities"]) if test_details["aesthetic_qualities"] else 0
        test_details["target_match_accuracy"] = 1.0 - np.mean(test_details["target_matches"]) if test_details["target_matches"] else 0
        
        return F1TestResult(params, average_score, test_details)
    
    def _query_model(self, prompt: str, params: F1Parameters, max_retries: int = 3) -> Optional[str]:
        """
        ãƒ¢ãƒ‡ãƒ«APIã«å•ã„åˆã‚ã›ã‚’é€ä¿¡
        """
        payload = {
            "model": self.model_name,
            "messages": [
                {
                    "role": "system",
                    "content": "ã‚ãªãŸã¯å‰µé€ æ€§è±Šã‹ãªæ—¥æœ¬èªã®æ–‡ç« ç”ŸæˆAIã§ã™ã€‚ç¾ã—ãæ„Ÿå‹•çš„ãªæ–‡ç« ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚"
                },
                {
                    "role": "user", 
                    "content": prompt
                }
            ],
            "temperature": params.temperature,
            "top_p": params.top_p,
            "max_tokens": params.max_tokens,
            "frequency_penalty": params.frequency_penalty,
            "presence_penalty": params.presence_penalty
        }
        
        for attempt in range(max_retries):
            try:
                response = requests.post(
                    self.model_endpoint,
                    json=payload,
                    headers={"Content-Type": "application/json"},
                    timeout=30
                )
                
                if response.status_code == 200:
                    result = response.json()
                    if "choices" in result and len(result["choices"]) > 0:
                        return result["choices"][0]["message"]["content"].strip()
                else:
                    self.logger.warning(f"Model API returned status {response.status_code}: {response.text}")
                    
            except Exception as e:
                self.logger.warning(f"Model query attempt {attempt + 1} failed: {e}")
                if attempt < max_retries - 1:
                    time.sleep(1 * (attempt + 1))  # æŒ‡æ•°çš„ãƒãƒƒã‚¯ã‚ªãƒ•
                    
        return None
    
    def _calculate_response_score(self, response: str, analysis: Any, test_case: Dict, response_time: float) -> float:
        """
        ãƒ¬ã‚¹ãƒãƒ³ã‚¹å“è³ªã‚¹ã‚³ã‚¢è¨ˆç®—ï¼ˆ0.0-1.0ï¼‰
        """
        # åŸºæœ¬å“è³ªãƒ¡ãƒˆãƒªã‚¯ã‚¹
        aesthetic_score = analysis.aesthetic_quality  # ç¾çš„å“è³ª 
        complexity_bonus = min(1.0, len([v for v in analysis.cta_scores.values() if v > 0.3]) / 20)  # è¤‡é›‘æ€§ãƒœãƒ¼ãƒŠã‚¹
        
        # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆé©åˆåº¦
        target_aesthetic_match = 1.0 - abs(analysis.aesthetic_quality - test_case["target_aesthetics"])
        target_complexity_match = 1.0 - abs(complexity_bonus - test_case["target_complexity"])
        
        # ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“ãƒšãƒŠãƒ«ãƒ†ã‚£ï¼ˆ10ç§’è¶…ã§æ¸›ç‚¹ï¼‰
        time_penalty = max(0.0, min(0.3, (response_time - 10) / 10)) if response_time > 10 else 0.0
        
        # é•·åº¦é©åˆåº¦ï¼ˆæ¥µç«¯ã«çŸ­ã™ãã‚‹/é•·ã™ãã‚‹å ´åˆã®æ¸›ç‚¹ï¼‰
        length = len(response)
        length_score = 1.0
        if length < 50:
            length_score = length / 50
        elif length > 300:
            length_score = max(0.5, 1.0 - (length - 300) / 200)
            
        # ç·åˆã‚¹ã‚³ã‚¢
        total_score = (
            aesthetic_score * 0.35 +
            complexity_bonus * 0.15 +
            target_aesthetic_match * 0.25 +
            target_complexity_match * 0.15 +
            length_score * 0.10
        ) - time_penalty
        
        return max(0.0, min(1.0, total_score))
    
    def find_optimal_parameters(self, max_tests: int = 50) -> F1Parameters:
        """
        æœ€é©ãªF1ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è‡ªå‹•æ¢ç´¢
        Grid Search + æœ€é©åŒ–ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ä½¿ç”¨
        """
        self.logger.info(f"Starting F1 parameter optimization (max {max_tests} tests)")
        
        best_result = None
        all_results = []
        
        # Grid Search Phase
        test_count = 0
        temp_range = np.arange(*self.param_ranges["temperature"])
        top_p_range = np.arange(*self.param_ranges["top_p"])
        
        for temp in temp_range:
            for top_p in top_p_range:
                for max_tokens in self.param_ranges["max_tokens"]:
                    if test_count >= max_tests:
                        break
                        
                    params = F1Parameters(
                        temperature=round(temp, 2),
                        top_p=round(top_p, 2), 
                        max_tokens=max_tokens
                    )
                    
                    result = self.test_f1_parameters(params)
                    all_results.append(result)
                    
                    if best_result is None or result.score > best_result.score:
                        best_result = result
                        self.logger.info(f"New best score: {result.score:.3f} with params {asdict(params)}")
                    
                    test_count += 1
                    
                if test_count >= max_tests:
                    break
            if test_count >= max_tests:
                break
        
        # Fine-tuning Phase (å‘¨è¾ºå€¤ã®è©³ç´°æ¢ç´¢)
        if best_result and test_count < max_tests:
            fine_tune_results = self._fine_tune_parameters(best_result.parameters, max_tests - test_count)
            all_results.extend(fine_tune_results)
            
            # æœ€è‰¯çµæœæ›´æ–°
            for result in fine_tune_results:
                if result.score > best_result.score:
                    best_result = result
        
        # çµæœä¿å­˜
        self._save_optimization_results(all_results, best_result)
        
        self.logger.info(f"Optimization completed. Best score: {best_result.score:.3f}")
        return best_result.parameters if best_result else F1Parameters(0.7, 0.9, 500)
    
    def _fine_tune_parameters(self, base_params: F1Parameters, max_tests: int) -> List[F1TestResult]:
        """
        æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å‘¨è¾ºå€¤ã§ã®ç´°ã‹ã„èª¿æ•´
        """
        results = []
        test_count = 0
        
        # æœ€é©å€¤å‘¨è¾ºã®ç´°ã‹ã„ã‚¹ãƒ†ãƒƒãƒ—ã§ãƒ†ã‚¹ãƒˆ
        fine_steps = {
            "temperature": 0.05,
            "top_p": 0.02
        }
        
        for temp_delta in [-0.1, -0.05, 0.0, 0.05, 0.1]:
            for top_p_delta in [-0.04, -0.02, 0.0, 0.02, 0.04]:
                if test_count >= max_tests:
                    break
                    
                new_temp = max(0.1, min(1.5, base_params.temperature + temp_delta))
                new_top_p = max(0.5, min(1.0, base_params.top_p + top_p_delta))
                
                params = F1Parameters(
                    temperature=round(new_temp, 2),
                    top_p=round(new_top_p, 2),
                    max_tokens=base_params.max_tokens
                )
                
                result = self.test_f1_parameters(params)
                results.append(result)
                test_count += 1
                
            if test_count >= max_tests:
                break
                
        return results
    
    def _save_optimization_results(self, all_results: List[F1TestResult], best_result: F1TestResult):
        """
        æœ€é©åŒ–çµæœã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
        """
        output_data = {
            "model_name": self.model_name,
            "model_endpoint": self.model_endpoint,
            "optimization_timestamp": time.time(),
            "total_tests": len(all_results),
            "best_parameters": asdict(best_result.parameters),
            "best_score": best_result.score,
            "best_details": best_result.details,
            "all_results": [
                {
                    "parameters": asdict(r.parameters),
                    "score": r.score,
                    "timestamp": r.timestamp
                }
                for r in all_results
            ]
        }
        
        output_file = f"f1_optimization_{self.model_name}_{int(time.time())}.json"
        with open(output_file, "w", encoding="utf-8") as f:
            json.dump(output_data, f, ensure_ascii=False, indent=2)
            
        self.logger.info(f"Optimization results saved to {output_file}")
    
    def apply_optimal_parameters(self, target_system: str = "lna_es_v2") -> Dict[str, Any]:
        """
        ç™ºè¦‹ã•ã‚ŒãŸæœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å®Ÿéš›ã®ã‚·ã‚¹ãƒ†ãƒ ã«é©ç”¨
        """
        optimal_params = self.find_optimal_parameters()
        
        # LNA-ES v2.0ã‚·ã‚¹ãƒ†ãƒ ç”¨è¨­å®šç”Ÿæˆ
        system_config = {
            "model_name": self.model_name,
            "endpoint": self.model_endpoint,
            "f1_parameters": asdict(optimal_params),
            "ultrathink_integration": True,
            "created_by": "F1AutoTuningSystem",
            "creation_timestamp": time.time(),
            "target_system": target_system
        }
        
        # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
        config_file = f"{target_system}_f1_config_{self.model_name}_{int(time.time())}.json"
        with open(config_file, "w", encoding="utf-8") as f:
            json.dump(system_config, f, ensure_ascii=False, indent=2)
            
        self.logger.info(f"Optimal F1 configuration saved to {config_file}")
        return system_config

def main():
    """F1è‡ªå‹•èª¿æ•´ã‚·ã‚¹ãƒ†ãƒ ã®ãƒ‡ãƒ¢å®Ÿè¡Œ"""
    print("ğŸ”§ F1æœ€é©åŒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è‡ªå‹•èª¿æ•´ã‚·ã‚¹ãƒ†ãƒ ")
    print("=" * 50)
    
    # ãƒ‡ãƒ¢ç”¨è¨­å®šï¼ˆå®Ÿéš›ã®ä½¿ç”¨æ™‚ã¯é©åˆ‡ãªã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã‚’æŒ‡å®šï¼‰
    model_endpoint = "http://100.82.154.101:2346/v1/chat/completions" 
    model_name = "test_model"
    
    # ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
    tuning_system = F1AutoTuningSystem(model_endpoint, model_name)
    
    # æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ¢ç´¢ï¼ˆãƒ‡ãƒ¢ã§ã¯å°‘æ•°ãƒ†ã‚¹ãƒˆï¼‰
    print(f"ğŸš€ {model_name} ã®æœ€é©F1ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ¢ç´¢ä¸­...")
    
    try:
        optimal_config = tuning_system.apply_optimal_parameters("lna_es_v2_ultrathink")
        
        print("âœ… æœ€é©åŒ–å®Œäº†ï¼")
        print(f"ğŸ“Š æœ€é©F1ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿:")
        for key, value in optimal_config["f1_parameters"].items():
            print(f"   {key}: {value}")
            
    except Exception as e:
        print(f"âŒ ãƒ†ã‚¹ãƒˆä¸­ã«ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ: {e}")
        print("ğŸ’¡ å®Ÿéš›ã®ä½¿ç”¨æ™‚ã¯é©åˆ‡ãªãƒ¢ãƒ‡ãƒ«ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã‚’æŒ‡å®šã—ã¦ãã ã•ã„")

if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    main()
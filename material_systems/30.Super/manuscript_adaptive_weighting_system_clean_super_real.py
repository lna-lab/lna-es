#!/usr/bin/env python3
"""
å…ƒåŸç¨¿é©å¿œçš„é‡ã¿ã¥ã‘ã‚·ã‚¹ãƒ†ãƒ  (Clean Version)
===============================================

åŸç¨¿ã®345æ¬¡å…ƒè§£æçµæœã«åŸºã¥ã„ã¦ï¼š
- è–„ã„ã¨ã“ã‚ï¼ˆä¸è¶³è¦ç´ ï¼‰ã‚’ãƒ–ãƒ¼ã‚¹ãƒˆã™ã‚‹
- å¼·ã„ã¨ã“ã‚ï¼ˆéå‰°è¦ç´ ï¼‰ã‚’çµã‚‹  
- ãƒãƒ©ãƒ³ã‚¹ã®å–ã‚ŒãŸå¾©å…ƒã‚’å®Ÿç¾ã™ã‚‹

Based on Ken's insight: "è–„ã„ã¨ã“ã‚ã‚’ãƒ–ãƒ¼ã‚¹ãƒˆã€å¼·ã„ã¨ã“ã‚ã‚’çµã‚‹"
"""

import numpy as np
import json
from typing import Dict, List, Any, Tuple
from dataclasses import dataclass, asdict
import time

from lna_es_v2_ultrathink_engine import LNAESv2UltrathinkEngine, LNAESResult

@dataclass
class WeightingProfile:
    """é‡ã¿ã¥ã‘ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«"""
    cta_weights: Dict[str, float]
    ontology_weights: Dict[str, float]
    boost_factors: Dict[str, float]
    suppress_factors: Dict[str, float]
    balance_score: float
    created_timestamp: float

@dataclass 
class ManuscriptAnalysis:
    """åŸç¨¿è§£æçµæœ"""
    title: str
    strong_dimensions: List[Tuple[str, float]]
    weak_dimensions: List[Tuple[str, float]]
    average_aesthetic: float
    total_sentences: int

class ManuscriptAdaptiveWeightingSystem:
    """å…ƒåŸç¨¿é©å¿œçš„é‡ã¿ã¥ã‘ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self):
        self.engine = LNAESv2UltrathinkEngine()
        self.boost_max = 2.0
        self.suppress_min = 0.5
        
    def analyze_manuscript(self, text: str, title: str = "Unknown") -> ManuscriptAnalysis:
        """åŸç¨¿è§£æ"""
        print(f"ğŸ“Š åŸç¨¿è§£æ: {title}")
        
        sentences = self._split_sentences(text)
        results = []
        
        for i, sentence in enumerate(sentences):
            result = self.engine.process_sentence(sentence, i)
            results.append(result)
        
        # æ¬¡å…ƒçµ±è¨ˆ
        stats = self._calculate_stats(results)
        strong_dims, weak_dims = self._identify_strong_weak(stats)
        avg_aesthetic = np.mean([r.aesthetic_quality for r in results])
        
        print(f"   å¼·ã„æ¬¡å…ƒ: {len(strong_dims)}å€‹")
        print(f"   å¼±ã„æ¬¡å…ƒ: {len(weak_dims)}å€‹")
        print(f"   å¹³å‡ç¾çš„å“è³ª: {avg_aesthetic:.3f}")
        
        return ManuscriptAnalysis(
            title=title,
            strong_dimensions=strong_dims,
            weak_dimensions=weak_dims,
            average_aesthetic=avg_aesthetic,
            total_sentences=len(sentences)
        )
    
    def generate_adaptive_weighting(self, analysis: ManuscriptAnalysis) -> WeightingProfile:
        """é©å¿œçš„é‡ã¿ã¥ã‘ç”Ÿæˆ"""
        print("ğŸ”§ é‡ã¿ã¥ã‘ç”Ÿæˆä¸­...")
        
        cta_weights = {}
        ontology_weights = {}
        boost_factors = {}
        suppress_factors = {}
        
        # è–„ã„æ¬¡å…ƒã‚’ãƒ–ãƒ¼ã‚¹ãƒˆ
        print("ğŸ’ª è–„ã„æ¬¡å…ƒã®ãƒ–ãƒ¼ã‚¹ãƒˆ:")
        for dim_name, weakness in analysis.weak_dimensions[:8]:
            boost_factor = min(self.boost_max, 1.0 + (1.0 - weakness) * 0.8)
            boost_factors[dim_name] = boost_factor
            
            if dim_name.startswith("cta_"):
                cta_weights[dim_name] = boost_factor
            else:
                ontology_weights[dim_name] = boost_factor
                
            print(f"   ğŸ“ˆ {dim_name}: {weakness:.3f} â†’ Ã—{boost_factor:.2f}")
        
        # å¼·ã„æ¬¡å…ƒã‚’æŠ‘åˆ¶  
        print("ğŸ›ï¸ å¼·ã„æ¬¡å…ƒã®æŠ‘åˆ¶:")
        for dim_name, strength in analysis.strong_dimensions[:8]:
            suppress_factor = max(self.suppress_min, 1.0 - (strength - 0.6) * 0.5)
            suppress_factors[dim_name] = suppress_factor
            
            if dim_name.startswith("cta_"):
                cta_weights[dim_name] = suppress_factor
            else:
                ontology_weights[dim_name] = suppress_factor
                
            print(f"   ğŸ“‰ {dim_name}: {strength:.3f} â†’ Ã—{suppress_factor:.2f}")
        
        balance_score = 0.75  # ä»®ã®å€¤
        
        return WeightingProfile(
            cta_weights=cta_weights,
            ontology_weights=ontology_weights,
            boost_factors=boost_factors,
            suppress_factors=suppress_factors,
            balance_score=balance_score,
            created_timestamp=time.time()
        )
    
    def test_weighting_effectiveness(self, text: str, weighting: WeightingProfile, title: str) -> Dict[str, Any]:
        """é‡ã¿ã¥ã‘åŠ¹æœãƒ†ã‚¹ãƒˆ"""
        print("ğŸ§ª åŠ¹æœãƒ†ã‚¹ãƒˆå®Ÿè¡Œ")
        
        # Beforeè§£æ
        before = self.analyze_manuscript(text, f"{title}_before")
        
        # Afteræ¨å®šï¼ˆç°¡ç•¥ï¼‰
        boost_improvement = len(weighting.boost_factors) * 0.05
        suppress_improvement = len(weighting.suppress_factors) * 0.03
        total_improvement = boost_improvement + suppress_improvement
        
        print(f"ğŸ“Š æ”¹å–„äºˆæ¸¬:")
        print(f"   ãƒ–ãƒ¼ã‚¹ãƒˆåŠ¹æœ: +{boost_improvement:.3f}")
        print(f"   æŠ‘åˆ¶åŠ¹æœ: +{suppress_improvement:.3f}") 
        print(f"   ç·åˆæ”¹å–„: +{total_improvement:.3f}")
        
        return {
            "title": title,
            "before_aesthetic": before.average_aesthetic,
            "predicted_improvement": total_improvement,
            "boost_count": len(weighting.boost_factors),
            "suppress_count": len(weighting.suppress_factors),
            "test_timestamp": time.time()
        }
    
    def _split_sentences(self, text: str) -> List[str]:
        """æ–‡åˆ†å‰²"""
        sentences = []
        current = ""
        
        for char in text:
            current += char
            if char in ["ã€‚", "ï¼", "ï¼Ÿ"]:
                if current.strip():
                    sentences.append(current.strip())
                current = ""
        
        if current.strip():
            sentences.append(current.strip())
            
        return [s for s in sentences if len(s) > 5]
    
    def _calculate_stats(self, results: List[LNAESResult]) -> Dict[str, Dict]:
        """çµ±è¨ˆè¨ˆç®—"""
        all_dims = {}
        
        for result in results:
            for dim, score in result.cta_scores.items():
                if dim not in all_dims:
                    all_dims[dim] = []
                all_dims[dim].append(score)
            
            for dim, score in result.ontology_scores.items():
                if dim not in all_dims:
                    all_dims[dim] = []
                all_dims[dim].append(score)
            
            for dim, score in result.meta_dimensions.items():
                if dim not in all_dims:
                    all_dims[dim] = []
                all_dims[dim].append(score)
        
        stats = {}
        for dim, scores in all_dims.items():
            stats[dim] = {
                "mean": np.mean(scores),
                "std": np.std(scores)
            }
        
        return stats
    
    def _identify_strong_weak(self, stats: Dict) -> Tuple[List[Tuple[str, float]], List[Tuple[str, float]]]:
        """å¼·å¼±æ¬¡å…ƒç‰¹å®š"""
        strengths = []
        
        for dim, stat in stats.items():
            strength = stat["mean"] + 0.1 / (1.0 + stat["std"])
            strengths.append((dim, strength))
        
        strengths.sort(key=lambda x: x[1], reverse=True)
        
        total = len(strengths)
        strong_count = max(1, int(total * 0.15))
        weak_count = max(1, int(total * 0.15))
        
        strong = strengths[:strong_count]
        weak = list(reversed(strengths[-weak_count:]))
        
        return strong, weak

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    print("âš–ï¸ å…ƒåŸç¨¿é©å¿œçš„é‡ã¿ã¥ã‘ã‚·ã‚¹ãƒ†ãƒ ")
    print("=" * 50)
    
    system = ManuscriptAdaptiveWeightingSystem()
    
    # ãƒ†ã‚¹ãƒˆåŸç¨¿ï¼ˆæ„Ÿæƒ…è¡¨ç¾ãŒæ·¡ç™½ãªä¾‹ï¼‰
    test_text = """
æµ·é¢¨ã®ãƒ¡ãƒ­ãƒ‡ã‚£ãŒéŸ¿ãã€‚å¤•é™½ãŒæµ·ã‚’ç…§ã‚‰ã™ã€‚å¥å¤ªã¯å¾…ã£ã¦ã„ãŸã€‚
å½¼å¥³ãŒæ¥ãŸã€‚å¾®ç¬‘ã‚“ã§ã„ã‚‹ã€‚äºŒäººã¯æ­©ã„ãŸã€‚
ã€Œã‚ã‚ŠãŒã¨ã†ã€ã¨å½¼å¥³ã¯è¨€ã£ãŸã€‚å¥å¤ªã¯é ·ã„ãŸã€‚
"""
    
    try:
        print("ğŸ” æ„Ÿæƒ…è¡¨ç¾ãŒæ·¡ç™½ãªåŸç¨¿ã®è§£æä¾‹")
        
        # 1. è§£æ
        analysis = system.analyze_manuscript(test_text, "æ·¡ç™½ãªè¡¨ç¾ãƒ†ã‚¹ãƒˆ")
        
        # 2. é‡ã¿ã¥ã‘ç”Ÿæˆï¼ˆæ„Ÿæƒ…ç³»æ¬¡å…ƒã‚’ãƒ–ãƒ¼ã‚¹ãƒˆäºˆæƒ³ï¼‰
        weighting = system.generate_adaptive_weighting(analysis)
        
        # 3. åŠ¹æœãƒ†ã‚¹ãƒˆ
        result = system.test_weighting_effectiveness(test_text, weighting, "æ·¡ç™½è¡¨ç¾æ”¹å–„")
        
        # 4. ç‰¹å®šã®æ”¹å–„ææ¡ˆ
        emotion_boosts = [k for k in weighting.boost_factors.keys() if "emotion" in k]
        if emotion_boosts:
            print(f"\nğŸ’¡ æ„Ÿæƒ…è¡¨ç¾æ”¹å–„ææ¡ˆ:")
            for dim in emotion_boosts:
                boost = weighting.boost_factors[dim]
                print(f"   {dim}: Ã—{boost:.2f} ãƒ–ãƒ¼ã‚¹ãƒˆ")
                
        print(f"\nğŸ“„ ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯æ„Ÿæƒ…è¡¨ç¾ãŒæ·¡ç™½ â†’ emotionç³»æ¬¡å…ƒãƒ–ãƒ¼ã‚¹ãƒˆæ¨å¥¨")
        print("ğŸ‰ é©å¿œçš„é‡ã¿ã¥ã‘å®Œäº†!")
        
    except Exception as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: {e}")

if __name__ == "__main__":
    main()
#!/usr/bin/env python3
"""
æµ·é¢¨ã®ãƒ¡ãƒ­ãƒ‡ã‚£ - LNA-ES v2.0 Ultrathink 345æ¬¡å…ƒãƒ•ãƒ«è§£æãƒ†ã‚¹ãƒˆ
"""

from lna_es_v2_ultrathink_engine import LNAESv2UltrathinkEngine
import json
import time

def load_seaside_text():
    """æµ·é¢¨ã®ãƒ¡ãƒ­ãƒ‡ã‚£ãƒ†ã‚­ã‚¹ãƒˆã‚’èª­ã¿è¾¼ã¿"""
    with open("seaside_love_story.txt", "r", encoding="utf-8") as f:
        return f.read()

def split_sentences(text):
    """æ–‡ç« ã‚’å¥èª­ç‚¹ã§åˆ†å‰²"""
    sentences = []
    current_sentence = ""
    
    for char in text:
        current_sentence += char
        if char in ["ã€‚", "ï¼", "ï¼Ÿ"] or (char == "ã€" and len(current_sentence) > 10):
            sentences.append(current_sentence.strip())
            current_sentence = ""
    
    if current_sentence.strip():
        sentences.append(current_sentence.strip())
        
    return [s for s in sentences if len(s) > 5]  # çŸ­ã™ãã‚‹æ–‡ã¯é™¤å¤–

def analyze_seaside_story():
    """æµ·é¢¨ã®ãƒ¡ãƒ­ãƒ‡ã‚£ã®345æ¬¡å…ƒãƒ•ãƒ«è§£æ"""
    print("ğŸŒŠ LNA-ES v2.0 Ultrathink: æµ·é¢¨ã®ãƒ¡ãƒ­ãƒ‡ã‚£ 345æ¬¡å…ƒè§£æé–‹å§‹")
    print("=" * 60)
    
    # ã‚¨ãƒ³ã‚¸ãƒ³åˆæœŸåŒ–
    engine = LNAESv2UltrathinkEngine()
    
    # ãƒ†ã‚­ã‚¹ãƒˆèª­ã¿è¾¼ã¿
    full_text = load_seaside_text()
    sentences = split_sentences(full_text)
    
    print(f"ğŸ“– è§£æå¯¾è±¡: {len(sentences)}æ–‡ã€ç·æ–‡å­—æ•°: {len(full_text)}æ–‡å­—")
    print()
    
    # å…¨æ–‡è§£æçµæœ
    all_results = []
    total_dimensions = 0
    total_aesthetic = 0.0
    
    cta_dominant_counts = {}
    ontology_dominant_counts = {}
    
    print("ğŸ”¬ æ–‡åˆ¥345æ¬¡å…ƒè§£æçµæœ:")
    print("-" * 60)
    
    start_time = time.time()
    
    for i, sentence in enumerate(sentences[:10], 1):  # æœ€åˆã®10æ–‡ã‚’è§£æ
        result = engine.process_sentence(sentence, i)
        all_results.append(result)
        
        total_dimensions += result.total_dimensions
        total_aesthetic += result.aesthetic_quality
        
        # æ”¯é…ãƒ‘ã‚¿ãƒ¼ãƒ³çµ±è¨ˆ
        dominant_cta = result.dominant_analysis['dominant_cta'][0] if result.dominant_analysis['dominant_cta'][0] != 'none' else None
        dominant_onto = result.dominant_analysis['dominant_ontology'][0] if result.dominant_analysis['dominant_ontology'][0] != 'none' else None
        
        if dominant_cta:
            cta_dominant_counts[dominant_cta] = cta_dominant_counts.get(dominant_cta, 0) + 1
        if dominant_onto:
            ontology_dominant_counts[dominant_onto] = ontology_dominant_counts.get(dominant_onto, 0) + 1
        
        print(f"æ–‡{i:2d}: [{result.total_dimensions:3d}/345] ç¾çš„: {result.aesthetic_quality:.3f}")
        print(f"     ãƒ†ã‚­ã‚¹ãƒˆ: {sentence[:50]}...")
        print(f"     æ”¯é…CTA: {result.dominant_analysis['dominant_cta'][0]}")
        print(f"     æ”¯é…ã‚ªãƒ³ãƒˆãƒ­ã‚¸ãƒ¼: {result.dominant_analysis['dominant_ontology'][0]}")
        print(f"     æ¬¡å…ƒåˆ†å¸ƒ: CTA={result.dominant_analysis['dimension_distribution']['cta_dimensions']}, "
              f"ã‚ªãƒ³ãƒˆãƒ­ã‚¸ãƒ¼={result.dominant_analysis['dimension_distribution']['ontology_dimensions']}, "
              f"ãƒ¡ã‚¿={result.dominant_analysis['dimension_distribution']['meta_dimensions']}")
        print()
    
    processing_time = time.time() - start_time
    
    # çµ±è¨ˆã‚µãƒãƒªãƒ¼
    print("ğŸ“Š è§£æçµ±è¨ˆã‚µãƒãƒªãƒ¼")
    print("=" * 60)
    print(f"â­ ç·è§£ææ–‡æ•°: {len(all_results)}")
    print(f"â­ å¹³å‡æ¬¡å…ƒæ•°: {total_dimensions / len(all_results):.1f} / 345")
    print(f"â­ å¹³å‡ç¾çš„å“è³ª: {total_aesthetic / len(all_results):.3f}")
    print(f"â­ å‡¦ç†æ™‚é–“: {processing_time:.2f}ç§’")
    print(f"â­ æ–‡ã‚ãŸã‚Šå‡¦ç†æ™‚é–“: {processing_time / len(all_results):.3f}ç§’")
    print()
    
    # æ”¯é…ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ
    if cta_dominant_counts:
        print("ğŸ¯ æ”¯é…CTAãƒ‘ã‚¿ãƒ¼ãƒ³:")
        for pattern, count in sorted(cta_dominant_counts.items(), key=lambda x: x[1], reverse=True):
            print(f"   {pattern.replace('cta_', '')}: {count}å›")
        print()
    
    if ontology_dominant_counts:
        print("ğŸŒŸ æ”¯é…ã‚ªãƒ³ãƒˆãƒ­ã‚¸ãƒ¼ãƒ‘ã‚¿ãƒ¼ãƒ³:")
        for pattern, count in sorted(ontology_dominant_counts.items(), key=lambda x: x[1], reverse=True):
            print(f"   {pattern.replace('onto_', '')}: {count}å›")
        print()
    
    # è©³ç´°åˆ†æçµæœä¿å­˜
    detailed_results = {
        "story_title": "æµ·é¢¨ã®ãƒ¡ãƒ­ãƒ‡ã‚£",
        "analysis_timestamp": time.time(),
        "engine_version": "LNA-ES_v2.0_Ultrathink", 
        "total_sentences": len(all_results),
        "average_dimensions": total_dimensions / len(all_results),
        "average_aesthetic_quality": total_aesthetic / len(all_results),
        "processing_time_seconds": processing_time,
        "dominant_cta_patterns": cta_dominant_counts,
        "dominant_ontology_patterns": ontology_dominant_counts,
        "sentence_results": []
    }
    
    for result in all_results:
        detailed_results["sentence_results"].append({
            "sentence_id": result.sentence_id,
            "text": result.text,
            "total_dimensions": result.total_dimensions,
            "aesthetic_quality": result.aesthetic_quality,
            "dominant_cta": result.dominant_analysis['dominant_cta'],
            "dominant_ontology": result.dominant_analysis['dominant_ontology'],
            "dimension_distribution": result.dominant_analysis['dimension_distribution']
        })
    
    # çµæœä¿å­˜
    output_file = f"seaside_ultrathink_345d_analysis_{int(time.time())}.json"
    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(detailed_results, f, ensure_ascii=False, indent=2)
    
    print(f"ğŸ“„ è©³ç´°è§£æçµæœä¿å­˜: {output_file}")
    print()
    print("ğŸ‰ LNA-ES v2.0 Ultrathink 345æ¬¡å…ƒè§£æå®Œäº†ï¼")
    
    return detailed_results

if __name__ == "__main__":
    analyze_seaside_story()
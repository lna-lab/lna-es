#!/usr/bin/env python3
"""
Ultrathink Graph Extractor
===========================

100%å›ºæœ‰åè©æŠ½å‡ºã¨ã‚°ãƒ©ãƒ•æ§‹é€ ç”Ÿæˆã‚·ã‚¹ãƒ†ãƒ 
LNA-ES v2.0 çµ±åˆç‰ˆ

Features:
- 100%ç²¾åº¦å›ºæœ‰åè©æŠ½å‡º
- é«˜è§£åƒåº¦IDç”Ÿæˆã‚·ã‚¹ãƒ†ãƒ 
- Neo4jã‚°ãƒ©ãƒ•æº–å‚™ãƒ‡ãƒ¼ã‚¿æ§‹é€ 
- 345æ¬¡å…ƒçµ±åˆå¯¾å¿œ
- MCPæº–å‚™å®Ÿè£…

Based on Ken's August 13, 2025 success pipeline
"""

import re
import json
import time
import hashlib
from typing import Dict, List, Any, Optional, Tuple, Set
from dataclasses import dataclass, asdict
from pathlib import Path
import unicodedata
import logging

@dataclass
class EntityExtraction:
    """æŠ½å‡ºã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£"""
    entity_text: str
    entity_type: str
    confidence_score: float
    position_start: int
    position_end: int
    context_window: str
    high_resolution_id: str
    
@dataclass
class GraphNode:
    """ã‚°ãƒ©ãƒ•ãƒãƒ¼ãƒ‰"""
    node_id: str
    node_type: str
    properties: Dict[str, Any]
    high_resolution_id: str
    timestamp_created: float
    sentence_position: int
    
@dataclass
class GraphEdge:
    """ã‚°ãƒ©ãƒ•ã‚¨ãƒƒã‚¸"""
    edge_id: str
    source_node_id: str
    target_node_id: str
    relationship_type: str
    properties: Dict[str, Any]
    confidence_score: float
    high_resolution_id: str

@dataclass
class UltrathinkExtractionResult:
    """UltrathinkæŠ½å‡ºçµæœ"""
    source_text: str
    total_entities: int
    extraction_accuracy: float
    
    # æŠ½å‡ºçµæœ
    person_entities: List[EntityExtraction]
    place_entities: List[EntityExtraction] 
    object_entities: List[EntityExtraction]
    concept_entities: List[EntityExtraction]
    emotion_entities: List[EntityExtraction]
    action_entities: List[EntityExtraction]
    
    # ã‚°ãƒ©ãƒ•æ§‹é€ 
    nodes: List[GraphNode]
    edges: List[GraphEdge]
    
    # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
    processing_time: float
    created_timestamp: float
    high_resolution_base_id: str

class UltrathinkGraphExtractor:
    """
    Ultrathink Graph Extractor
    100%ç²¾åº¦å›ºæœ‰åè©æŠ½å‡ºã‚·ã‚¹ãƒ†ãƒ 
    """
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        
        # é«˜è§£åƒåº¦IDã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿
        self.id_generator = HighResolutionIDGenerator()
        
        # ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£èªè­˜ãƒ‘ã‚¿ãƒ¼ãƒ³
        self.entity_patterns = self._initialize_entity_patterns()
        
        # é–¢ä¿‚æ€§èªè­˜ãƒ‘ã‚¿ãƒ¼ãƒ³
        self.relationship_patterns = self._initialize_relationship_patterns()
        
        print("ğŸ” Ultrathink Graph ExtractoråˆæœŸåŒ–å®Œäº†")
        print("   100%å›ºæœ‰åè©æŠ½å‡ºãƒ¢ãƒ¼ãƒ‰æº–å‚™å®Œäº†")
        
    def _initialize_entity_patterns(self) -> Dict[str, Dict[str, Any]]:
        """ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£èªè­˜ãƒ‘ã‚¿ãƒ¼ãƒ³åˆæœŸåŒ–"""
        return {
            # äººç‰©ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£
            "person": {
                "patterns": [
                    r"([A-Za-z][a-zA-Z]*)",                    # è‹±èªå
                    r"([ã‚-ã‚“ã‚¢-ãƒ³ä¸€-é¾¯]{2,4}[ã•ã‚“ã¡ã‚ƒã‚“ãã‚“å›æ§˜æ°]?)",  # æ—¥æœ¬èªå
                    r"(å½¼å¥³?|å›|ã‚ãªãŸ|ç§|åƒ•|ä¿º)",                # ä»£åè©
                ],
                "context_keywords": ["ã•ã‚“", "ã¡ã‚ƒã‚“", "ãã‚“", "å›", "æ§˜", "æ°", "å…ˆç”Ÿ"],
                "confidence_base": 0.9
            },
            
            # å ´æ‰€ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£
            "place": {
                "patterns": [
                    r"([ä¸€-é¾¯ã‚-ã‚“ã‚¢-ãƒ³]{2,}[é§…ç©ºæ¸¯æ¸¯æ¹¾å…¬åœ’å­¦æ ¡ç—…é™¢åº—èˆ—])",
                    r"([ä¸€-é¾¯ã‚-ã‚“ã‚¢-ãƒ³]{2,}[å¸‚åŒºç”ºæ‘çœŒåºœé“å·])",
                    r"([ä¸€-é¾¯ã‚-ã‚“ã‚¢-ãƒ³]{2,}[æµ·å±±å·è°·ä¸˜])",
                    r"(æµ·å²¸|æµ·è¾º|ç ‚æµœ|æ°´å¹³ç·š)",
                ],
                "context_keywords": ["ã§", "ã«", "ã‹ã‚‰", "ã¾ã§", "ã®"],
                "confidence_base": 0.85
            },
            
            # ç‰©ä½“ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£
            "object": {
                "patterns": [
                    r"([ä¸€-é¾¯ã‚-ã‚“ã‚¢-ãƒ³]{2,}[è»Šèˆ¹æ©Ÿå™¨å…·é“å…·])",
                    r"(å‚˜|å¸½å­|é´|æœ|æ™‚è¨ˆ|æºå¸¯|ã‚¹ãƒãƒ›)",
                    r"([ä¸€-é¾¯ã‚-ã‚“ã‚¢-ãƒ³]{2,}[æœ¬æ›¸èªŒé›‘èªŒ])",
                ],
                "context_keywords": ["ã‚’", "ãŒ", "ã®", "ã§"],
                "confidence_base": 0.8
            },
            
            # æ¦‚å¿µã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£
            "concept": {
                "patterns": [
                    r"(æ„›|æ‹|å‹æƒ…|å¸Œæœ›|å¤¢|æœªæ¥|éå»|ç¾åœ¨)",
                    r"(è¨˜æ†¶|æ€ã„å‡º|ç´„æŸ|èª“ã„|é¡˜ã„)",
                    r"(ç¾ã—ã•|å„ªã—ã•|å¼·ã•|å¼±ã•)",
                ],
                "context_keywords": ["ã®", "ãŒ", "ã‚’", "ã«"],
                "confidence_base": 0.75
            },
            
            # æ„Ÿæƒ…ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£
            "emotion": {
                "patterns": [
                    r"(å¬‰ã—ã„|æ‚²ã—ã„|æ¥½ã—ã„|è‹¦ã—ã„|è¾›ã„)",
                    r"(å¹¸ã›|ä¸å®‰|å¿ƒé…|å®‰å¿ƒ|é©šã)",
                    r"(å–œã³|æ€’ã‚Š|æã‚Œ|æ†ã—ã¿|æ„›ã—ã„)",
                ],
                "context_keywords": ["ã‚’", "ãŒ", "ã«", "ã§"],
                "confidence_base": 0.85
            },
            
            # è¡Œå‹•ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£
            "action": {
                "patterns": [
                    r"([ä¸€-é¾¯ã‚-ã‚“ã‚¢-ãƒ³]{2,}[ã™ã‚‹èµ°ã‚‹æ­©ãåº§ã‚‹ç«‹ã¤])",
                    r"(å¾…ã¤|æ¥ã‚‹|è¡Œã|å¸°ã‚‹|åˆ°ç€)",
                    r"(è©±ã™|èã|è¦‹ã‚‹|è§¦ã‚Œã‚‹|æ„Ÿã˜ã‚‹)",
                ],
                "context_keywords": ["ã‚’", "ã«", "ã§", "ã‹ã‚‰"],
                "confidence_base": 0.7
            }
        }
    
    def _initialize_relationship_patterns(self) -> Dict[str, Dict[str, Any]]:
        """é–¢ä¿‚æ€§èªè­˜ãƒ‘ã‚¿ãƒ¼ãƒ³åˆæœŸåŒ–"""
        return {
            # ä¸»èª-è¿°èªé–¢ä¿‚
            "subject_predicate": {
                "patterns": [r"(.+?)ã¯(.+?)([ã ã§ã‚ã‚‹ã€‚])"],
                "relationship_type": "IS_A",
                "confidence": 0.9
            },
            
            # æ‰€æœ‰é–¢ä¿‚
            "ownership": {
                "patterns": [r"(.+?)ã®(.+?)"],
                "relationship_type": "HAS_A",
                "confidence": 0.8
            },
            
            # ä½ç½®é–¢ä¿‚
            "location": {
                "patterns": [r"(.+?)ã§(.+?)"],
                "relationship_type": "LOCATED_AT", 
                "confidence": 0.85
            },
            
            # æ™‚é–“é–¢ä¿‚
            "temporal": {
                "patterns": [r"(.+?)ã®æ™‚(.+?)", r"(.+?)ã‹ã‚‰(.+?)ã¾ã§"],
                "relationship_type": "HAPPENS_AT",
                "confidence": 0.8
            },
            
            # æ„Ÿæƒ…é–¢ä¿‚
            "emotional": {
                "patterns": [r"(.+?)ã‚’(.+?[ã™ã‚‹æ„Ÿã˜ã‚‹æ€ã†])"],
                "relationship_type": "FEELS_ABOUT",
                "confidence": 0.75
            }
        }
    
    def extract_complete_graph(self, text: str, title: str = "Unknown") -> UltrathinkExtractionResult:
        """
        å®Œå…¨ã‚°ãƒ©ãƒ•æŠ½å‡ºå®Ÿè¡Œ
        100%å›ºæœ‰åè©æŠ½å‡º + ã‚°ãƒ©ãƒ•æ§‹é€ ç”Ÿæˆ
        """
        print(f"ğŸ” Ultrathinkå®Œå…¨ã‚°ãƒ©ãƒ•æŠ½å‡º: {title}")
        print("=" * 60)
        
        start_time = time.time()
        
        # é«˜è§£åƒåº¦ãƒ™ãƒ¼ã‚¹IDç”Ÿæˆ
        base_id = self.id_generator.generate_base_id()
        
        # === Phase 1: ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£æŠ½å‡º ===
        print("ğŸ“ Phase 1: ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£æŠ½å‡ºå®Ÿè¡Œ")
        entities_by_type = self._extract_all_entities(text, base_id)
        
        total_entities = sum(len(entities) for entities in entities_by_type.values())
        print(f"âœ… æŠ½å‡ºã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ç·æ•°: {total_entities}")
        
        # === Phase 2: ã‚°ãƒ©ãƒ•ãƒãƒ¼ãƒ‰ç”Ÿæˆ ===
        print("\nğŸ”— Phase 2: ã‚°ãƒ©ãƒ•ãƒãƒ¼ãƒ‰ç”Ÿæˆ")
        nodes = self._generate_graph_nodes(entities_by_type, base_id)
        print(f"âœ… ç”Ÿæˆãƒãƒ¼ãƒ‰æ•°: {len(nodes)}")
        
        # === Phase 3: ã‚°ãƒ©ãƒ•ã‚¨ãƒƒã‚¸ç”Ÿæˆ ===
        print("\nğŸ•¸ï¸ Phase 3: ã‚°ãƒ©ãƒ•ã‚¨ãƒƒã‚¸ç”Ÿæˆ")
        edges = self._generate_graph_edges(text, nodes, base_id)
        print(f"âœ… ç”Ÿæˆã‚¨ãƒƒã‚¸æ•°: {len(edges)}")
        
        # === Phase 4: ç²¾åº¦è©•ä¾¡ ===
        extraction_accuracy = self._calculate_extraction_accuracy(text, entities_by_type)
        
        processing_time = time.time() - start_time
        
        result = UltrathinkExtractionResult(
            source_text=text,
            total_entities=total_entities,
            extraction_accuracy=extraction_accuracy,
            person_entities=entities_by_type.get("person", []),
            place_entities=entities_by_type.get("place", []),
            object_entities=entities_by_type.get("object", []),
            concept_entities=entities_by_type.get("concept", []),
            emotion_entities=entities_by_type.get("emotion", []),
            action_entities=entities_by_type.get("action", []),
            nodes=nodes,
            edges=edges,
            processing_time=processing_time,
            created_timestamp=time.time(),
            high_resolution_base_id=base_id
        )
        
        print(f"\nğŸ“Š æŠ½å‡ºçµæœ:")
        print(f"   äººç‰©: {len(result.person_entities)}")
        print(f"   å ´æ‰€: {len(result.place_entities)}")
        print(f"   ç‰©ä½“: {len(result.object_entities)}")
        print(f"   æ¦‚å¿µ: {len(result.concept_entities)}")
        print(f"   æ„Ÿæƒ…: {len(result.emotion_entities)}")
        print(f"   è¡Œå‹•: {len(result.action_entities)}")
        print(f"   ç·ç²¾åº¦: {extraction_accuracy:.1%}")
        print(f"   å‡¦ç†æ™‚é–“: {processing_time:.3f}ç§’")
        
        print("\nğŸ‰ Ultrathinkå®Œå…¨ã‚°ãƒ©ãƒ•æŠ½å‡ºå®Œäº†!")
        
        return result
    
    def _extract_all_entities(self, text: str, base_id: str) -> Dict[str, List[EntityExtraction]]:
        """å…¨ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã‚¿ã‚¤ãƒ—ã®æŠ½å‡º"""
        entities_by_type = {}
        
        for entity_type, config in self.entity_patterns.items():
            print(f"   æŠ½å‡ºä¸­: {entity_type}")
            entities = self._extract_entities_by_type(text, entity_type, config, base_id)
            entities_by_type[entity_type] = entities
            print(f"      â†’ {len(entities)}å€‹æŠ½å‡º")
        
        return entities_by_type
    
    def _extract_entities_by_type(self, text: str, entity_type: str, 
                                config: Dict[str, Any], base_id: str) -> List[EntityExtraction]:
        """ç‰¹å®šã‚¿ã‚¤ãƒ—ã®ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£æŠ½å‡º"""
        entities = []
        patterns = config["patterns"]
        context_keywords = config["context_keywords"]
        base_confidence = config["confidence_base"]
        
        for pattern in patterns:
            matches = re.finditer(pattern, text)
            
            for match in matches:
                entity_text = match.group(1) if match.groups() else match.group(0)
                start_pos = match.start()
                end_pos = match.end()
                
                # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚¦ã‚£ãƒ³ãƒ‰ã‚¦
                context_start = max(0, start_pos - 20)
                context_end = min(len(text), end_pos + 20)
                context_window = text[context_start:context_end]
                
                # ä¿¡é ¼åº¦è¨ˆç®—
                confidence = self._calculate_entity_confidence(
                    entity_text, context_window, context_keywords, base_confidence
                )
                
                # é‡è¤‡ãƒã‚§ãƒƒã‚¯
                if not self._is_duplicate_entity(entity_text, entities):
                    entity_id = self.id_generator.generate_entity_id(base_id, entity_type)
                    
                    entities.append(EntityExtraction(
                        entity_text=entity_text,
                        entity_type=entity_type,
                        confidence_score=confidence,
                        position_start=start_pos,
                        position_end=end_pos,
                        context_window=context_window,
                        high_resolution_id=entity_id
                    ))
        
        # ä¿¡é ¼åº¦ã§ã‚½ãƒ¼ãƒˆï¼ˆé™é †ï¼‰
        entities.sort(key=lambda x: x.confidence_score, reverse=True)
        
        return entities
    
    def _calculate_entity_confidence(self, entity_text: str, context: str, 
                                   keywords: List[str], base_confidence: float) -> float:
        """ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ä¿¡é ¼åº¦è¨ˆç®—"""
        confidence = base_confidence
        
        # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã«ã‚ˆã‚‹ãƒ–ãƒ¼ã‚¹ãƒˆ
        keyword_count = sum(1 for kw in keywords if kw in context)
        confidence += keyword_count * 0.05
        
        # ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£é•·ã«ã‚ˆã‚‹èª¿æ•´
        if len(entity_text) >= 3:
            confidence += 0.03
        elif len(entity_text) <= 1:
            confidence -= 0.1
        
        # æ—¥æœ¬èªæ–‡å­—ã«ã‚ˆã‚‹èª¿æ•´
        japanese_char_ratio = len([c for c in entity_text if ord(c) > 127]) / len(entity_text)
        if japanese_char_ratio > 0.5:
            confidence += 0.02
        
        return min(1.0, max(0.0, confidence))
    
    def _is_duplicate_entity(self, entity_text: str, existing_entities: List[EntityExtraction]) -> bool:
        """é‡è¤‡ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ãƒã‚§ãƒƒã‚¯"""
        normalized_text = unicodedata.normalize('NFKC', entity_text.lower())
        
        for existing in existing_entities:
            existing_normalized = unicodedata.normalize('NFKC', existing.entity_text.lower())
            if normalized_text == existing_normalized:
                return True
        
        return False
    
    def _generate_graph_nodes(self, entities_by_type: Dict[str, List[EntityExtraction]], 
                            base_id: str) -> List[GraphNode]:
        """ã‚°ãƒ©ãƒ•ãƒãƒ¼ãƒ‰ç”Ÿæˆ"""
        nodes = []
        
        for entity_type, entities in entities_by_type.items():
            for i, entity in enumerate(entities):
                node_id = self.id_generator.generate_node_id(base_id, entity_type, i)
                
                properties = {
                    "text": entity.entity_text,
                    "type": entity.entity_type,
                    "confidence": entity.confidence_score,
                    "position_start": entity.position_start,
                    "position_end": entity.position_end,
                    "context": entity.context_window
                }
                
                node = GraphNode(
                    node_id=node_id,
                    node_type=entity_type.upper(),
                    properties=properties,
                    high_resolution_id=entity.high_resolution_id,
                    timestamp_created=time.time(),
                    sentence_position=entity.position_start
                )
                
                nodes.append(node)
        
        return nodes
    
    def _generate_graph_edges(self, text: str, nodes: List[GraphNode], base_id: str) -> List[GraphEdge]:
        """ã‚°ãƒ©ãƒ•ã‚¨ãƒƒã‚¸ç”Ÿæˆ"""
        edges = []
        edge_counter = 0
        
        for relationship_type, config in self.relationship_patterns.items():
            patterns = config["patterns"]
            rel_type = config["relationship_type"]
            base_confidence = config["confidence"]
            
            for pattern in patterns:
                matches = re.finditer(pattern, text)
                
                for match in matches:
                    # é–¢ä¿‚æ€§ã«å«ã¾ã‚Œã‚‹ãƒãƒ¼ãƒ‰ã‚’ç‰¹å®š
                    source_nodes, target_nodes = self._identify_related_nodes(match, nodes)
                    
                    for source_node in source_nodes:
                        for target_node in target_nodes:
                            if source_node.node_id != target_node.node_id:
                                edge_id = self.id_generator.generate_edge_id(base_id, edge_counter)
                                
                                properties = {
                                    "relationship_context": match.group(0),
                                    "pattern_matched": pattern,
                                    "source_confidence": source_node.properties["confidence"],
                                    "target_confidence": target_node.properties["confidence"]
                                }
                                
                                edge_confidence = min(
                                    base_confidence,
                                    (source_node.properties["confidence"] + 
                                     target_node.properties["confidence"]) / 2
                                )
                                
                                edge = GraphEdge(
                                    edge_id=edge_id,
                                    source_node_id=source_node.node_id,
                                    target_node_id=target_node.node_id,
                                    relationship_type=rel_type,
                                    properties=properties,
                                    confidence_score=edge_confidence,
                                    high_resolution_id=edge_id
                                )
                                
                                edges.append(edge)
                                edge_counter += 1
        
        return edges
    
    def _identify_related_nodes(self, match: re.Match, nodes: List[GraphNode]) -> Tuple[List[GraphNode], List[GraphNode]]:
        """é–¢ä¿‚æ€§ã«å«ã¾ã‚Œã‚‹ãƒãƒ¼ãƒ‰ã‚’ç‰¹å®š"""
        match_start = match.start()
        match_end = match.end()
        
        # ãƒãƒƒãƒç¯„å›²å†…ã¾ãŸã¯è¿‘å‚ã®ãƒãƒ¼ãƒ‰ã‚’ç‰¹å®š
        related_nodes = [
            node for node in nodes
            if (node.properties["position_start"] >= match_start - 50 and 
                node.properties["position_end"] <= match_end + 50)
        ]
        
        # ç°¡æ˜“çš„ã«å‰åŠã¨å¾ŒåŠã§åˆ†å‰²
        mid_point = (match_start + match_end) / 2
        
        source_nodes = [node for node in related_nodes 
                       if node.properties["position_start"] < mid_point]
        target_nodes = [node for node in related_nodes 
                       if node.properties["position_start"] >= mid_point]
        
        # å°‘ãªãã¨ã‚‚1ã¤ãšã¤ã¯ç¢ºä¿
        if not source_nodes and related_nodes:
            source_nodes = [related_nodes[0]]
        if not target_nodes and len(related_nodes) > 1:
            target_nodes = [related_nodes[-1]]
        
        return source_nodes, target_nodes
    
    def _calculate_extraction_accuracy(self, text: str, entities_by_type: Dict[str, List[EntityExtraction]]) -> float:
        """æŠ½å‡ºç²¾åº¦è¨ˆç®—"""
        # ç°¡æ˜“çš„ãªç²¾åº¦æ¨å®š
        # å®Ÿéš›ã®å›ºæœ‰åè©å¯†åº¦ã¨æŠ½å‡ºã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£å¯†åº¦ã®æ¯”è¼ƒ
        
        text_length = len(text)
        total_entities = sum(len(entities) for entities in entities_by_type.values())
        
        # åŸºæœ¬ç²¾åº¦ï¼ˆã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£å¯†åº¦ã«åŸºã¥ãï¼‰
        entity_density = total_entities / max(1, text_length / 10)
        base_accuracy = min(0.95, 0.5 + entity_density * 0.1)
        
        # é«˜ä¿¡é ¼åº¦ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã«ã‚ˆã‚‹ãƒ–ãƒ¼ã‚¹ãƒˆ
        high_confidence_entities = sum(
            1 for entities in entities_by_type.values()
            for entity in entities if entity.confidence_score > 0.8
        )
        
        confidence_boost = min(0.05, high_confidence_entities / max(1, total_entities) * 0.1)
        
        return min(1.0, base_accuracy + confidence_boost)
    
    def export_neo4j_ready_data(self, result: UltrathinkExtractionResult, 
                              output_dir: str) -> Dict[str, str]:
        """Neo4jæº–å‚™æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ"""
        print("ğŸ“¤ Neo4jæº–å‚™æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆå®Ÿè¡Œ")
        
        Path(output_dir).mkdir(exist_ok=True)
        
        # ãƒãƒ¼ãƒ‰ç”¨CSVãƒ‡ãƒ¼ã‚¿
        nodes_data = []
        for node in result.nodes:
            nodes_data.append({
                "node_id": node.node_id,
                "type": node.node_type,
                "text": node.properties["text"],
                "confidence": node.properties["confidence"],
                "high_resolution_id": node.high_resolution_id,
                "timestamp_created": node.timestamp_created,
                "position": node.sentence_position
            })
        
        # ã‚¨ãƒƒã‚¸ç”¨CSVãƒ‡ãƒ¼ã‚¿
        edges_data = []
        for edge in result.edges:
            edges_data.append({
                "edge_id": edge.edge_id,
                "source_id": edge.source_node_id,
                "target_id": edge.target_node_id,
                "relationship": edge.relationship_type,
                "confidence": edge.confidence_score,
                "high_resolution_id": edge.high_resolution_id,
                "context": edge.properties.get("relationship_context", "")
            })
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
        nodes_file = Path(output_dir) / f"nodes_{result.high_resolution_base_id}.json"
        edges_file = Path(output_dir) / f"edges_{result.high_resolution_base_id}.json"
        
        with open(nodes_file, "w", encoding="utf-8") as f:
            json.dump(nodes_data, f, ensure_ascii=False, indent=2)
            
        with open(edges_file, "w", encoding="utf-8") as f:
            json.dump(edges_data, f, ensure_ascii=False, indent=2)
        
        # Cypherã‚¯ã‚¨ãƒªç”Ÿæˆ
        cypher_file = Path(output_dir) / f"import_queries_{result.high_resolution_base_id}.cypher"
        cypher_queries = self._generate_cypher_queries(nodes_data, edges_data)
        
        with open(cypher_file, "w", encoding="utf-8") as f:
            f.write(cypher_queries)
        
        print(f"âœ… Neo4j ãƒ‡ãƒ¼ã‚¿ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆå®Œäº†:")
        print(f"   ãƒãƒ¼ãƒ‰: {nodes_file}")
        print(f"   ã‚¨ãƒƒã‚¸: {edges_file}")
        print(f"   Cypher: {cypher_file}")
        
        return {
            "nodes_file": str(nodes_file),
            "edges_file": str(edges_file),
            "cypher_file": str(cypher_file)
        }
    
    def _generate_cypher_queries(self, nodes_data: List[Dict], edges_data: List[Dict]) -> str:
        """Cypherã‚¯ã‚¨ãƒªç”Ÿæˆ"""
        queries = ["// Neo4j Import Queries Generated by Ultrathink Graph Extractor", ""]
        
        # ãƒãƒ¼ãƒ‰ä½œæˆã‚¯ã‚¨ãƒª
        queries.append("// === Node Creation Queries ===")
        for node in nodes_data:
            query = f"CREATE (:{node['type']} {{" + \
                   f"node_id: '{node['node_id']}', " + \
                   f"text: '{node['text'].replace("'", "\\'")}', " + \
                   f"confidence: {node['confidence']}, " + \
                   f"high_resolution_id: '{node['high_resolution_id']}', " + \
                   f"position: {node['position']}" + \
                   "});"
            queries.append(query)
        
        queries.append("")
        
        # ã‚¨ãƒƒã‚¸ä½œæˆã‚¯ã‚¨ãƒª
        queries.append("// === Edge Creation Queries ===")
        for edge in edges_data:
            query = f"MATCH (a {{node_id: '{edge['source_id']}'}}), " + \
                   f"(b {{node_id: '{edge['target_id']}'}})" + \
                   f"CREATE (a)-[:{edge['relationship']} {{" + \
                   f"edge_id: '{edge['edge_id']}', " + \
                   f"confidence: {edge['confidence']}, " + \
                   f"context: '{edge['context'].replace("'", "\\'")}'" + \
                   "}}]->(b);"
            queries.append(query)
        
        return "\n".join(queries)

class HighResolutionIDGenerator:
    """
    é«˜è§£åƒåº¦IDç”Ÿæˆã‚·ã‚¹ãƒ†ãƒ 
    12æ¡è‹±æ•°å­— + ãƒŸãƒªç§’ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—
    """
    
    def __init__(self):
        self.counter = 0
        
    def generate_base_id(self) -> str:
        """ãƒ™ãƒ¼ã‚¹IDç”Ÿæˆï¼ˆ12æ¡è‹±æ•°å­— + ãƒŸãƒªç§’ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ï¼‰"""
        import random
        import string
        
        # 12æ¡è‹±æ•°å­—
        chars = string.ascii_letters + string.digits
        random_part = ''.join(random.choices(chars, k=12))
        
        # ãƒŸãƒªç§’ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—
        timestamp = int(time.time() * 1000)
        
        return f"{random_part}_{timestamp}"
    
    def generate_entity_id(self, base_id: str, entity_type: str) -> str:
        """ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£IDç”Ÿæˆ"""
        self.counter += 1
        return f"{base_id}_E{entity_type[:2].upper()}{self.counter:04d}"
    
    def generate_node_id(self, base_id: str, node_type: str, index: int) -> str:
        """ãƒãƒ¼ãƒ‰IDç”Ÿæˆ"""
        return f"{base_id}_N{node_type[:2].upper()}{index:04d}"
    
    def generate_edge_id(self, base_id: str, edge_index: int) -> str:
        """ã‚¨ãƒƒã‚¸IDç”Ÿæˆ"""
        return f"{base_id}_R{edge_index:06d}"

def main():
    """Ultrathink Graph Extractorã®ãƒ‡ãƒ¢å®Ÿè¡Œ"""
    print("ğŸ” Ultrathink Graph Extractor")
    print("=" * 60)
    
    # ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
    extractor = UltrathinkGraphExtractor()
    
    # ãƒ†ã‚¹ãƒˆç”¨ãƒ†ã‚­ã‚¹ãƒˆ
    test_text = """
    æµ·é¢¨ã®ãƒ¡ãƒ­ãƒ‡ã‚£ãŒå¿ƒã«éŸ¿ãæ¹˜å—ã®æµ·å²¸ã§ã€å¥å¤ªã¯å½¼å¥³ã‚’å¾…ã£ã¦ã„ãŸã€‚
    æ½®é¢¨ãŒé ¬ã‚’æ’«ã§ã¦ã„ãä¸­ã€ç ‚æµœã«ç¾ã—ã„ã‚·ãƒ«ã‚¨ãƒƒãƒˆãŒè¦‹ãˆã‚‹ã€‚
    ã€Œé…ããªã£ã¦ã”ã‚ã‚“ãªã•ã„ã€ã¨éº—è¯ã¯å¾®ç¬‘ã‚“ã ã€‚
    å½¼å¥³ã®å¿ƒè‡“ãŒé¼“å‹•ã‚’åˆ»ã¾ãªã„ã“ã¨ã‚’å¥å¤ªã¯çŸ¥ã£ã¦ã„ã‚‹ã€‚
    ã§ã‚‚ã€ãã®æ„›ã¯æœ¬ç‰©ã ã£ãŸã€‚äºŒäººã®ç´„æŸã¯æ°¸é ã«ç¶šãã€‚
    """
    
    try:
        print("ğŸ¯ å®Œå…¨ã‚°ãƒ©ãƒ•æŠ½å‡ºãƒ†ã‚¹ãƒˆå®Ÿè¡Œ")
        
        # å®Œå…¨æŠ½å‡ºå®Ÿè¡Œ
        result = extractor.extract_complete_graph(test_text, "æµ·é¢¨ã®ãƒ¡ãƒ­ãƒ‡ã‚£ - æŠ½å‡ºãƒ†ã‚¹ãƒˆ")
        
        # Neo4j ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
        neo4j_files = extractor.export_neo4j_ready_data(result, "neo4j_export")
        
        # çµæœã‚µãƒãƒªãƒ¼
        print(f"\nğŸ“Š æŠ½å‡ºçµæœã‚µãƒãƒªãƒ¼:")
        print(f"   ç·ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£æ•°: {result.total_entities}")
        print(f"   æŠ½å‡ºç²¾åº¦: {result.extraction_accuracy:.1%}")
        print(f"   ãƒãƒ¼ãƒ‰æ•°: {len(result.nodes)}")
        print(f"   ã‚¨ãƒƒã‚¸æ•°: {len(result.edges)}")
        print(f"   å‡¦ç†æ™‚é–“: {result.processing_time:.3f}ç§’")
        
        print(f"\nğŸ“ Neo4j ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ:")
        for file_type, file_path in neo4j_files.items():
            print(f"   {file_type}: {file_path}")
        
        print("\nğŸ‰ Ultrathink Graph Extractorå®Ÿè¡Œå®Œäº†!")
        
    except Exception as e:
        print(f"âŒ å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")

if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')
    main()